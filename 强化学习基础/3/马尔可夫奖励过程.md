# 马尔可夫奖励过程

马尔可夫奖励过程（Markov Reward Process，MRP）是马尔可夫过程的扩展，它在马尔可夫过程的基础上引入了**奖励机制和折扣因子**，用于描述带有回报的随机过程。

## 定义

一个马尔可夫奖励过程由以下四个要素组成：

1. **状态集合 \(S\)**：表示所有可能的状态。
2. **转移概率矩阵 \(P\)**：描述从一个状态转移到另一个状态的概率。
3. **奖励函数 \(R\)**：定义了在状态转移过程中智能体获得的即时奖励。
4. **折扣因子 \(\gamma\)**：取值范围为 \([0, 1]\)，用于对未来的奖励进行折扣，以平衡当前奖励和未来奖励的重要性。

## 核心概念

- **回报（Return）**：从某一时刻 \(t\) 开始，后续所有奖励的折扣总和，假设时刻

$t$后的奖励序列$R_t, R_{t+1}, R_{t+2}, \dots, R_T$，则回报为：
  \[
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T-t-1} R_T
  \]
  其中，$T$是最终时刻，$\gamma$ 是折扣因子，越往后得到的奖励，折扣越多。这说明我们**更希望得到现有的奖
  励，对未来的奖励要打折扣**。当我们有了回报之后，就可以定义状态的价值了，就是状态价值函数（state-value function）。对于马尔可夫奖励过程，状态价值函数被定义成回报的期望，即

- **状态价值函数（State Value Function）**：表示从状态 \(s\) 开始的期望回报，定义为：
  \[
  v(s) = \mathbb{E}[G_t | S_t = s] =[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots+\gamma^{T-t-1} R_T | S_t = s]
  \]

其中，$G_t$是之前定义的**折扣回报（discounted return）**。我们对$G_t$取了一个期望，期望就是从这个状态始，我们可能获得多大的价值。所以期望也可以看成未来可能获得奖励的当前价值的表现，就是当我们进入某一个状态后，我们现在有多大的价值。

我们使用折扣因子的原因如下。

- 第一，有些马尔可夫过程是带环的，它并不会终结，我们想避免无穷的奖励。
- 第二，我们并不能建立完美的模拟环境的模型，我们对未来的评估不一定是准

确的，我们不一定完全信任模型，因为这种不确定性，所以我们对未来的评估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。第三，如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。
最后，我们也更想得到即时奖励。有些时候可以把折扣
因子设为0($\gamma = 0$)，我们就只关注当前的奖励。我们也可以把折扣因子设为1($\gamma = 1$)，对未来的奖励并没有打折扣，未来获得的奖励与当前获得的奖励是一样的。折扣因子可以作为强化学习智能体的一个超参数（hyperparameter）来进行调整，通过调整折扣因子，我们可以得到不同动作的智能体。

贝尔曼方程（Bellman Equation）是马尔可夫奖励过程（MRP）和马尔可夫决策过程（MDP）中的一个基本方程，它描述了状态价值函数的递归关系。贝尔曼方程由理查德·贝尔曼（Richard Bellman）提出，是动态规划和强化学习中的一个关键概念。

## 贝尔曼方程定义

对于马尔可夫奖励过程（MRP），贝尔曼方程可以表示为：
\[
v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]
\]
其中：

- \(v(s)\) 是状态 \(s\) 的价值函数。
- \(R_{t+1}\) 是从状态 \(s\) 转移到下一个状态 \(S_{t+1}\) 时获得的即时奖励。
- \(\gamma\) 是折扣因子，取值范围为 \([0, 1]\)。
- \(S_{t+1}\) 是从状态 \(s\) 转移后的下一个状态。

### 展开形式

对于离散状态空间，贝尔曼方程可以展开为：
\[
v(s) = \sum_{s'} P(s, s') [R(s, s') + \gamma v(s')]
\]
其中：

- \(P(s, s')\) 是从状态 \(s\) 转移到状态 \(s'\) 的概率。
- \(R(s, s')\) 是从状态 \(s\) 转移到状态 \(s'\) 时获得的即时奖励。

### 贝尔曼方程与MDP

对于马尔可夫决策过程（MDP），贝尔曼方程需要考虑不同的动作选择。对于每个状态 \(s\) 和动作 \(a\)，贝尔曼方程可以表示为：
\[
q(s, a) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a]
\]
其中：

- \(q(s, a)\) 是状态 \(s\) 下采取动作 \(a\) 的动作价值函数。
- \(A_t = a\) 表示在时间 \(t\) 采取的动作是 \(a\)。

### 重要性

贝尔曼方程是求解MRP和MDP问题的基础。通过求解贝尔曼方程，可以得到每个状态的价值函数，从而找到最优策略。在强化学习中，贝尔曼方程被广泛应用于各种算法，如值迭代（Value Iteration）、策略迭代（Policy Iteration）和Q学习（Q-Learning）等。

### 总结

贝尔曼方程是描述状态价值函数递归关系的基本方程，它在马尔可夫奖励过程和马尔可夫决策过程中起着至关重要的作用。通过求解贝尔曼方程，可以找到最优策略，使智能体在环境中获得最大的累积奖励。