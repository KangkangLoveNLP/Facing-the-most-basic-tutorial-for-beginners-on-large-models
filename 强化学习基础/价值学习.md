# 价值学习

## 动作价值函数

**基础概念**：

- 奖励$R$:agent每作出一个动作，环境都会更新，并且会给出一个奖励$R$,每一个奖励$R$都依赖于前一个状态和动作。
- 回报（return）$U_t$:回报依赖从$T$时刻开始的所有的动作和所有的状态

$$ U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \dots + \gamma^{T-t} R_T $$

**回报的特点**:

- **长期回报**：一个状态$s$的回报$U_t$，是所有$t$时刻的奖励的和，所以$U_t$是一个**长期**的回报。回报$U_t$依赖从$T$时刻开始的所有的动作和所有的状态
- **动作的随机性**：动作的随机性，即在状态$s$下，采取动作$a$的概率是不确定的。动作的随机性来自策略$\pi$的抽样。

$$ \mathbb{P}[ A = a | S = s ] = \pi(a | s)$$

- **状态的随机性**：状态的随机性来自状态转移函数$p$,新的状态市场函数$p$中抽样得到的

$$ \mathbb{P}[ S = s' | S = s, A = a ] = p(s' | s, a)$$

- **随机性**： 动作和状态都是随机的，因此$U_t$也是一个随机变量。

### 对$U_t$求期望得到动作价值函数

因为回报$U_t$是一个随机变量，我们要消除它的随机性,只留下$s_t$和$a_t$两个变量，我们可以对它求期望。所以$U_t$的期望值可以表示为：
$$ Q_\pi(s_t,a_t) = \mathbb{E}[U_t | S_t = s_t, A_t = a_t]$$

这样我们就得到**动作价值函数(Action-Value Function)**.
动作价值函数只和**策略$\pi$、当前的状态和动作($s_t$和$a_t$)有关**。

### 动作价值函数的意义

动作价值函数可以反映出当前状态$s_t$下，采取动作$a_t$的好坏程度（期望）。

### 最优动作价值函数(Optimal Action-Value Function)

要想消除策略$\pi$对动作价值函数的随机性，我们可以定义一个**最优动作价值函数(Optimal Action-Value Function)**,对$Q_\pi$求最大化：

$$ Q^* (s, a) = {max}_\pi Q_\pi (s_t, a_t) $$

最优动作价值函数是一个**最优**的动作价值函数，它表示了当前状态$s_t$下，采取动作$a_t$的最优情况，**它与策略$\pi$无关**。要使ag**ent在当前状态$s_t$下采取动作$a_t$，得到的回报$U_t$最多就是$Q^* (s_t, a_t)$**
。
$Q^*$函数可以指导agent做决策，它可以为所有的动作打分，并且选择分数最高的作为动作。

### 如何理解$Q^*$函数

我们可以理解$Q^*$函数，它表示了当前状态$s_t$下，采取动作$a_t$的平均回报，你可以把它当作一个先知，它可以告诉你每一支股票的期望收益。

## 价值学习的基本思想

虽然$Q^*$函数非常厉害，但是实际上我们并没有$Q^*$函数，我们只能通过学习得到$Q^*$函数。我们可以通过学习一个函数来近似$Q^*$函数。这就是价值学习的基本思想。

### Deep Q-Network(DQN)

使用一个深度学习的神经网络来近似$Q^*$函数。

我们可以使用一个函数来代表这个神经网络
$$ Q(s,a;W) \rightarrow Q^* (s,a)$$

其中：

- W: 神经网络的参数
- s: 神经网络的输入，状态。
- 神经网络的输出是很多数值，这些数值是对每个可能的动作的打分。
  
举个例子，在超级玛丽中，游戏可以有三个动作，分别是向左，向右，向上。
我们将游戏的一帧通过卷积、特征提取输入到DQN中，然后得到三个动作的打分。

- 向上： 2000
- 向右： 1000
- 向右： 1000

很明显这时我们应该采取的动作是向上。

### DQN玩游戏的具体流程

一个使用深度Q网络（DQN）来玩电子游戏的基本流程。流程从当前状态开始，通过选择动作、接收奖励、更新状态，不断循环进行。以下是详细的步骤描述：

1. **开始于当前状态**：流程从当前状态 $S_t$ 开始。

2. **选择动作**：在状态 $S_t$ 下，通过最大化Q值函数 $Q(S_t, a; \mathbf{w})$ 来选择一个动作 $a_t$。这里的 $\mathbf{w}$ 表示Q值函数的参数。

3. **执行动作并观察结果**：执行动作 $a_t$ 后，状态转移函数$P$观察到下一个状态 $S_{t+1}$ 和获得的奖励 $r_t$。奖励可以用来更更新参数

4. **更新状态**：状态更新到 $S_{t+1}$。

5. **重复选择动作**：在新的状态 $S_{t+1}$ 下，再次通过最大化Q值函数来选择动作 $a_{t+1}$。

6. **继续循环**：这个过程不断重复，即在每个新的状态 $S_{t+2}, S_{t+3}, \ldots$ 下，都通过最大化Q值函数来选择最优动作 $a_{t+2}, a_{t+3}, \ldots$，并观察到相应的奖励 $r_{t+1}, r_{t+2}, \ldots$。知道游戏结束

7. **状态转移**：每个状态 $S_{t+1}, S_{t+2}, S_{t+3}, \ldots$ 都是根据当前状态和动作，通过概率分布 $p(\cdot | S_t, a_t)$ 来确定的。

这个流程展示了如何通过不断学习和选择最优动作来最大化累积奖励，这是强化学习中DQN算法的核心思想。

### 如何训练DQN——TD算法(Temporal Difference Algorithm，时差算法)

**实际问题类比**：

假如我想要从广州坐高铁到西安，我们可以先使用$Q(W)$函数进行预测。假设预测值为**q = 15小时** ，注意**此时模型还在训练，预测的值可能是完全错误的**，然后我们实际坐车是**y = 10个小时**，这样我们就得到一个**误差**，我们希望这个误差越小越好。

得到损失
$$Loss = \frac{1}{2}(q - y)^2$$
梯度$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial q} \frac{\partial q}{\partial w}  =  (q - y) \frac{\partial Q(w)}{\partial w}$$

梯度下降：
$$w_{t+1} = w_t - \alpha \frac{\partial L}{\partial w} |_{w = w_t}$$

**提出问题**：上述算法必须完成整个旅途才能对模型做一次更新，假如没有完成全程，如何进行更行：

假如我从广州做到西安，使用$Q(W)$函数进行预测。预测值为**q = 15小时** ，但是我坐了三个个小时到了武汉，就不坐了，这时我该如何进行更新模型呢？

我到了武汉，还可以使用$Q(W)$函数进行预测，预测值为**q = 10小时**，

- 模型最初的预测： $Q(W)$ = 15小时
- 我们可以使用武汉的时间进行更新模型的预测： $Q(W)$ = 3小时(实际的时间) +  10小时（再进行预测的时间 = 13小时，我们将这个值称为**TD target**

**TD target**与最开始预测的**q** 同样也有一个误差，我们也可以利用这个误差进行更新模型。

TD target ： $$y = 13$$
Loss : $$Loss = \frac{1}{2}(Q(w) - y)^2$$

梯度$$\frac{\partial L}{\partial w} =  (15 - 13) \frac{\partial Q(w)}{\partial w}$$

梯度下降：
$$w_{t+1} = w_t - \alpha \frac{\partial L}{\partial w} |_{w = w_t}$$

这个算法就是时差算法，**它通过不断更新模型来最小化损失函数，从而提高模型的预测性能。**

换成是强化学习也是同样的道理，我们不需要完成游戏全程也能更新参数。

## 将TD算法应用到DQN中

再上述例子中，我们可以发现有以下的等式；

$$T_{西安 \rightarrow{广州}} \approx T_{西安 \rightarrow{武汉}} + T_{武汉 \rightarrow{广州}}$$

其中$T_{西安 \rightarrow{广州}}$是模型预测的，$T_{西安 \rightarrow{武汉}}$是实际时间，$T_{武汉 \rightarrow{广州}}$是再进行预测的时间。

那么在深度强化学习中，也应该有一个等式。

$$ Q(s_t,a_t;W) \approx R_t + \gamma Q(s_{t+1},a_{t+1};W)$$

模型的输出$Q(s_t,a_t;W)$是对回报$\mathbb{E}(U_t)$做出的估计，
模型的输出$Q(s_{t+1},a_{t+1};W)$是对回报$\mathbb{E}(U_{t+1})$做出的估计，

所以：
$$ Q(s_t,a_t;W) \approx \mathbb{E}(R_t + \gamma Q(s_{t+1},a_{t+1};W))$$

更新过程：

- 开始的预测 Prediction: $Q(S_t, a_t; \mathbf{w}_t)$.
- TD target:
  $$
  y_t = r_t + \gamma \cdot Q(S_{t+1}, a_{t1+}; \mathbf{w}_t) = r_t + \gamma \cdot \max_a Q(S_{t+1}, a; \mathbf{w}_t).
  $$
- Loss: $$L_t = \frac{1}{2} [Q(S_t, a_t; \mathbf{w}) - y_t]^2$$.
- 梯度更新Gradient descent: $$\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha \cdot \frac{\partial L_t}{\partial \mathbf{w}} \big|_{\mathbf{w}=\mathbf{w}_t}$$.
  