# 策略学习(Policy-Based Reinforcement Learning)

我们可以用一个神经网络来近似一个策略函数，叫做**Policy Network**。可以用来控制agent的动作。

## 一、策略函数

$\pi (a|s)$,他是一个概率密度函数。

- 策略函数的输入是状态
- 输出是一个概率分布，给每个动作$a$一个概率值

### 1.1 策略函数输出的例子

我们可以举一个超级玛丽的例子，把当前的状态$s$作为输入，输出三个动作$a_{left,right,jump}$的概率。是一个三维向量。
$\pi(left|s) = 0.2$
$\pi(right|s) = 0.8$
$\pi(jump|s) = 0.7$

有了概率agent会进行一次随机抽样，**三个动作都会被抽到，但是概率越大，被抽到的概率越大**。这里会有一个误区，认为agent只会随机抽到概率最大的动作。

## 二、使用神经网络来近似策略函数：Policy Network ,策略网络

和价值学习一样，我们无法直接得到策略函数，但我们可以使用深度学习中的神经网络通过不断迭代来近似得到。
$$\pi(a|s,\theta) \rightarrow \pi(a|s)$$

$\theta$是神经网络的参数，可以通过梯度下降来更新。

### 2.1 策略网络运行的例子

还是超级玛丽的游戏作为例子。

- **我们首先对游戏的画面进行采样，得到某一帧的画面作为状态$s_t$**
- **我们对这一帧画面进行卷积、特征提取，得到一个特征向量**
- **我们将这个特征向量作为输入，通过神经网络后再进行softmax得到三个动作$a_{left,right,jump}$的概率。**
- **agent会对得到的概率进行采样，得到一个动作$a_t$**。

### 2.2需要的几个概念

1. **回报$U_t$(Discounted Return)**

$$ U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} +···$$
回报依赖从$T$时刻开始的所有的动作和所有的状态，是所有奖励的折扣和，$\gamma$是折扣系数。
2. **动作价值函数 $Q_{\pi}$(Action Value Function)**

$$ Q_{\pi}(s_t,a_t) = \mathbb{E}[U_t|s_t,a_t] $$

$Q_{\pi}$仅仅依赖当前时刻的状态和动作和策略函数$\pi$,动作价值函数可以评价在状态$s_t$下，执行动作$a_t$的回报是多少。它可以评估动作的好坏。
3. **状态价值函数 $V_{\pi}$(State Value Function)**

$$ V_{\pi}(s_t) = \mathbb{E}_A[Q_{\pi(s_t,A)}] $$
**$V_{\pi}$是$Q_{\pi}$的期望，$V_{\pi}$仅仅依赖当前时刻的状态和策略函数$\pi$,它可以评估状态的好坏。**,它越大，说明当前环境的胜算越大。

**如果给定状态$s_t$，$V_{\pi}(s_t|A)$可以评估策略$\pi$的好坏/**

如果A是离散的变量，那么我们可以将上述的公式展开：

$$ V_{\pi}(s_t) = \sum_{a} \pi(a|s_t) Q_{\pi}(s_t,a) $$

### 2.3神经网络近似策略函数

我们使用神经网络来近似策略函数，神经网络的输入是状态，输出是动作的概率。
$$ V_{\pi}(s_t) = V_{\pi}(s;{\theta}) = \sum_{a} \pi(a|s;{\theta}) Q_{\pi}(s,a) $$

其中，{\theta}是神经网络的参数。

## 三、策略学习的主要思想

由状态价值函数可以知道，**给定环境$s$,我们就可以评估一个策略函数$\pi$的好坏。$V(s;{\theta})$的值越大，策略函数就越好，我们可以改变参数$\theta$来使得$V(s;{\theta})$的值变大。**

### 3.1 目标函数的定义

由以上思想，我们可以定义要更新的目标函数：

$$ J(\theta) = \mathbb{E}_S[V_{\pi}(s_t;{\theta})] $$

**我们将状态$S$作为随机变量使用期望消去，这样我们定义的目标函数就只剩下${\theta}$**

$J(\theta)$越大，我们的策略函数就越好

### 3.2策略梯度算法

**大概思想**：

1. **首先我们从环境中采样得到一个状态$s_t$**
2. **我们可以根据这个状态带入到$V(s;{\theta})$中，计算他的梯度**
3. **进行梯度上升**：$$\theta = \theta + \beta \frac{\partial V(s;\theta)}{\partial \theta}$$

$\\beta $就是学习率
它是一个随机梯度，随机性来源于$s$

**$\frac{\partial V(s;\theta)}{\partial \theta}$被称为策略梯度。**

### 3.2策略梯度的推导

$$
\begin{split}
\frac{\partial V(s;\theta)}{\partial \theta}
&=\frac{ \partial {\sum_{a}\pi(a|s;\theta)}} {\partial \theta } \\
&=\sum_{a} \frac{\partial \pi(a|s;\theta) \cdot Q_{\pi}(s,a)}{\partial \theta}\\
&=\sum_{a} \frac{\partial \pi(a|s;\theta)}{\partial \theta} \cdot  Q_{\pi}(s,a)\\
\end{split}
$$
**如果动作$A$是离散的，直接带入就能把策略梯度算出来，但是实际运用中并不会直接使用这个公式，而是使用策略梯度的蒙特卡洛近似。**

### 3.3策略梯度的两个公式推导

$$
\begin{split}
\frac{\partial V(s;\theta)}{\partial \theta}
&=\sum_{a} \frac{\partial \pi(a|s;\theta)}{\partial \theta} \cdot  Q_{\pi}(s,a)\\
&= \sum_a \pi(a|s;\theta) \cdot \frac{\partial \log \pi(a|s;\theta)}{\partial \theta} \cdot Q_\pi(s, a)
\end{split}
$$

这一步从上往下不好推导，我们可以从下往上推导：

$$\frac{\partial \log[\pi(\theta)]}{\partial \theta} = \frac{1}{\pi(\theta)} \cdot \frac{\partial \pi(\theta)}{\partial \theta}$$
$$\Rightarrow \pi(\theta) \cdot \frac{\partial \log[\pi(\theta)]}{\partial \theta} = \pi(\theta) \cdot \frac{1}{\pi(\theta)} \cdot \frac{\partial \pi(\theta)}{\partial \theta} = \frac{\partial \pi(\theta)}{\partial \theta}$$
这样我们就推导
$$ \pi(\theta) \cdot \frac{\partial \log[\pi(\theta)]}{\partial \theta} = \frac{\partial \pi(\theta)}{\partial \theta}$$

我们接第一个推导继续推导

$$
\begin{split}
\frac{\partial V(s;\theta)}{\partial \theta}
&= \sum_a \frac{\partial \pi(a|s;\theta)}{\partial \theta} \cdot Q_\pi(s, a)\\
&= \sum_a \pi(a|s;\theta) \cdot \frac{\partial \log \pi(a|s;\theta)}{\partial \theta} \cdot Q_\pi(s, a)\\
&= \mathbb{E}_{A \sim \pi(\bullet|s;\theta)} \left[ \frac{\partial \log \pi(A|s;\theta)}{\partial \theta} \cdot Q_\pi(s, A) \right]
\end{split}
$$

**实际上，下面中形式是等价的**。

$$\frac{\partial V(s;\theta)}{\partial \theta}  = \sum_{a} \frac{\partial \pi(a|s;\theta)}{\partial \theta} \cdot  Q_{\pi}(s,a)$$

上面的公式对离散的动作空间适用，比如我们的超级玛丽游戏，我们只有三个动作。
$$\frac{\partial V(s;\theta)}{\partial \theta}  = \mathbb{E}_{A \sim \pi(\bullet|s;\theta)} \left[ \frac{\partial \log \pi(A|s;\theta)}{\partial \theta} \cdot Q_\pi(s, A) \right]$$
上面的公式对连续的动作空间使用，比如说对动作空间是零到一之间的所有实数，我们就用蒙特卡洛近似的公式。

## 四、策略梯度算法的的步骤分解

### 4.1 动作空间离散的情况

首先我们对于每一个$a \in \mathcal{A}$都带入到策略梯度公式中，记为$\mathbf{f}(a, \theta)$
$$\mathbf{f}(a, \theta) = \frac{\partial \pi(a|s;\theta)}{\partial \theta} \cdot Q_\pi(s, a)$$

计算出每个离散值的$\mathbf{f}(a, \theta)$，我们可以将他们累加起来，得到策略梯度公式
$$\frac{\partial V(s;\theta)}{\partial \theta} = \mathbf{f}(\text{"left"}, \theta) + \mathbf{f}(\text{"right"}, \theta) + \mathbf{f}(\text{"up"}, \theta)$$

但是如果动作空间是连续的，那么将会由无穷多个动作，这时在进行累加就比较困难，如果我们选择积分的话，由于策略函数是一个神经网络，那么我们无法直接计算出策略梯度，所以我们需要使用蒙特卡洛方法来计算。

### 4.2使用蒙特卡洛近似来计算策略梯度

蒙特卡洛方法的基本思想是通过大量随机抽样来近似期望值。对于强化学习中的价值函数估计，蒙特卡洛方法通过多次抽样，用随机样本来近似期望来更新模型。

**公式 2:**
$$
\frac{\partial V(s;\theta)}{\partial \theta} = \mathbb{E}_{A \sim \pi(\cdot|s;\theta)} \left[ \frac{\partial \log \pi(A|s;\theta)}{\partial \theta} \cdot Q_\pi(s, A) \right]
$$
这个公式表示状态价值函数 $V(s;\theta)$ 关于参数 $\theta$ 的梯度可以通过期望来计算。期望是在动作 $A$ 根据策略 $\pi(\cdot|s;\theta)$ 采样的情况下计算的，其中 $Q_\pi(s, A)$ 是在状态 $s$ 下采取动作 $A$ 的期望回报。

### 步骤解释

1. **随机采样动作**：
   - 根据概率密度函数 $\pi(\cdot|s;\theta)$ 随机采样一个动作 $\hat{a}$。这意味着从策略定义的动作分布中抽取一个动作。

2. **计算 $g(\hat{a}, \theta)$**：
   - 计算 $g(\hat{a}, \theta) = \frac{\partial \log \pi(\hat{a}|s;\theta)}{\partial \theta} \cdot Q_\pi(s, \hat{a})$。这里，$\frac{\partial \log \pi(\hat{a}|s;\theta)}{\partial \theta}$ 是策略的对数关于参数 $\theta$ 的梯度，$Q_\pi(s, \hat{a})$ 是在状态 $s$ 下采取动作 $\hat{a}$ 的期望回报。

3. **使用 $g(\hat{a}, \theta)$ 作为策略梯度的近似**：
   - 使用 $g(\hat{a}, \theta)$ 作为策略梯度 $\frac{\partial V(s;\theta)}{\partial \theta}$ 的近似。这意味着通过单个动作的采样和计算得到的 $g(\hat{a}, \theta)$ 可以用来估计整个策略梯度。

这种方法对于离散的也是适用的。

### 4.3 总结策略梯度算法

1. **观察状态 $s_t$**：
   - 在时间步 $t$，观察或接收环境的当前状态 $s_t$。

2. **根据策略 $\pi(\cdot | s_t; \theta_t)$ 随机采样动作 $a_t$**：
   - 根据当前策略 $\pi$（由参数 $\theta_t$ 定义）在状态 $s_t$ 下的概率分布，随机选择一个动作 $a_t$。

3. **计算 $q_t \approx Q_\pi(s_t, a_t)$（某种估计）**：
   - 计算或估计在状态 $s_t$ 下采取动作 $a_t$ 的期望回报 $Q_\pi(s_t, a_t)$。这里 $q_t$ 是这个期望回报的估计值。

4. **对策略网络求导**：
   - 计算策略网络关于参数 $\theta$ 的梯度 $d_{\theta,t}$，即 $\frac{\partial \log \pi(a_t | s_t, \theta)}{\partial \theta}$ 在 $\theta = \theta_t$ 时的值。这个梯度表示策略参数如何影响选择特定动作 $a_t$ 的概率。

5. **（近似）策略梯度**：
   - 计算策略梯度的近似值 $g(a_t, \theta_t) = q_t \cdot d_{\theta,t}$。这里，$q_t$ 是步骤3中计算的期望回报的估计值，$d_{\theta,t}$ 是步骤4中计算的梯度。

6. **更新策略网络**：
   - 使用梯度上升方法更新策略网络的参数 $\theta$。更新公式为 $\theta_{t+1} = \theta_t + \beta \cdot g(a_t, \theta_t)$，其中 $\beta$ 是学习率，控制更新步长的大小。

## 五、动作价值函数$Q_{\pi}(s_t,a_t)$

其实我们一直没有说明动作价值函数$Q_{\pi}(s_t,a_t)$是什么，该如何得到。

我们并不知道$Q_{\pi}(s_t,a_t)$，并没有办法计算这个函数值，但是我们可以近似得到这个函数的值 $q_t \approx Q_\pi(s_t, a_t)$，我们有两个方法来近似$q_t$

### 5.1 方法一：reinforce

REINFORCE算法的核心思想是通过采样来估计策略梯度，并使用这个估计值来更新策略参数。

1. **生成轨迹**：
   - 玩完一局游戏并生成轨迹：$s_1, a_1, r_1, s_2, a_2, r_2, \ldots, s_T, a_T, r_T$。这里，$s_t$ 是时间步 $t$ 的状态，$a_t$ 是时间步 $t$ 的动作，$r_t$ 是时间步 $t$ 的奖励，$T$ 是游戏的总时间步数。

2. **计算折扣回报**：
   - 计算折扣回报 $u_t = \sum_{k=t}^T \gamma^{k-t} r_k$，对于所有 $t$。这里，$\gamma$ 是折扣因子，用于权衡未来奖励的重要性。

3. **近似动作价值函数**：
   - 由于 $Q_\pi(s_t, a_t) = \mathbb{E}[U_t]$，我们可以使用 $u_t$ 来近似 $Q_\pi(s_t, a_t)$。即 $q_t = u_t$。

### 解释

- **轨迹生成**：通过与环境交互生成完整的轨迹，记录每个时间步的状态、动作和奖励。
- **折扣回报**：计算从当前时间步 $t$ 到游戏结束的所有未来奖励的加权和，权重由折扣因子 $\gamma$ 决定。
- **近似动作价值**：使用折扣回报 $u_t$ 作为动作价值函数 $Q_\pi(s_t, a_t)$ 的估计值 $q_t$。

这种方法的优点是简单且易于实现，但可能存在高方差的问题，因为折扣回报 $u_t$ 可能对单个样本的波动非常敏感。为了降低方差，可以使用基线方法或优势函数等技术进行改进。

### 5.2 方法二：使用神经网络近似

这个方法比较复杂，我会放到下一期进行讲解。
