# Actor 和 Critic方法

Actor是策略网络，可以看作是运动员， Critic是价值网络，可以看作是裁判员。
将价值学习和策略学习结合起来的算法

## 一、Actor-Critic-Method算法原理

### 1.1回顾公式

1. **定义：状态价值函数（Action-value function）**：
\[ V_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_t | S_t = s \right] \]
状态价值函数（State-Value Function）是强化学习中的一个基本概念，用于评估在给定策略 \(\pi\) 下，从某个状态 \(s\) 开始，未来能够获得的期望回报。状态价值函数 \(V_{\pi}(s)\) 可以定义为

简单来说状态价值函数可以评估状态的好坏，它表示在给定策略下，从当前状态开始，未来能够获得的期望回报，它是动作价值函数$Q_{\pi}$的期望

1. **状态价值函数的另一个公式**：

   - 公式：\[ V_{\pi}(s) = \sum_a \pi(a|s) \cdot Q_{\pi}(s, a) \]
   - 这里，\( V_{\pi}(s) \) 表示在策略 \(\pi\) 下，状态 \(s\) 的值函数。它是所有可能动作 \(a\) 的加权和，权重是策略 \(\pi\) 在状态 \(s\) 下选择动作 \(a\) 的概率 \(\pi(a|s)\)，而 \(Q_{\pi}(s, a)\) 是在状态 \(s\) 采取动作 \(a\) 后的价值。

2. **策略网络（Policy network，actor）**：
   - 使用神经网络 \(\pi(a|s; \theta)\) 来近似策略 \(\pi(a|s)\)。
   - \(\theta\) 是神经网络的可训练参数。
   - - 相当于一个运动员，根据状态选择动作,它可以根据裁判员的打分来改进自己
3. **价值网络（Value network，critic）**：
   - 使用神经网络 \(q(s, a; w)\) 来近似 \(Q_{\pi}(s, a)\)。
   - \(w\) 是神经网络的可训练参数。
   - 相当于一个裁判，给运动员进行打分
这种近似方法在强化学习中用于估计策略和价值函数，以便在复杂环境中进行决策。

$$
V_{\pi}(s) = \sum_a \pi(a|s) \cdot Q_{\pi}(s, a) \approx \sum_a \pi(a|s; \theta) \cdot q(s, a; w)
$$

同时训练这两个网络就称为Actor-Critic-Method

### 1.2 状态价值函数的神经网络近似

$$
V_{\pi}(s) \approx V_{\pi}(s;w,\theta) = \sum_a \pi(a|s; \theta) \cdot q(s, a; w)
$$

我们可以将状态价值函数 \(V_{\pi}(s)\) 的神经网络 \(V_{\pi}(s;w,\theta)\) 的参数 \(\theta\,w\) 分别称为策略网络的参数和价值网络的参数。更新的时候是两个参数同时更新的。但是二者的更新目标是不一样的。

1. 策略网络的参数$\theta$的更新目标

- 更新$\theta$的目标是使$V_{\pi}(s;w,\theta)$ 的值增加，$V_{\pi}(s;w,\theta)$是对状态$s$和策略 $\pi$的评价，如果将状态$s$固定，$V_{\pi}(s;w,\theta)$的值增加，说明策略$\pi$越好，因此需要更新$\theta$。
- 在更新参数$\theta$时，对策略网络的监督是由价值网络提供的。

1. 价值网络的参数$w$的更新目标

- 更新$w$的目标是让自己的打分越来越精准。
- 裁判是靠环境给的奖励来进行更新，裁判的目标是让他的打分接近环境给的奖励。

### 1.3 Actor-Critic-Method的具体步骤



- 状态值函数 \( V(s; \theta, w) \) 是通过神经网络来近似的。
- 公式：\[ V(s; \theta, w) = \sum_a \pi(a|s; \theta) \cdot q(s, a; w) \]
  - 这里，\(\pi(a|s; \theta)\) 是策略网络，参数为 \(\theta\)，用于选择动作 \(a\)。
  - \(q(s, a; w)\) 是价值网络，参数为 \(w\)，用于评估在状态 \(s\) 采取动作 \(a\) 的价值。


1. **观察状态 \(s_t\)**：
   - 在时间步 \(t\)，观察当前状态 \(s_t\)。

2. **随机采样动作 \(a_t\)**：
   - 根据策略网络 \(\pi(\cdot | s_t; \theta_t)\) 随机采样动作 \(a_t\)。

3. **执行动作 \(a_t\) 并观察新状态 \(s_{t+1}\) 和奖励 \(r_t\)**：
   - 执行动作 \(a_t\)，观察结果状态 \(s_{t+1}\) 和获得的奖励 \(r_t\)。

4. **更新 \(w\)（在价值网络中）使用时间差分（TD）**：
   - 使用时间差分误差来更新价值网络的参数 \(w\)。时间差分误差是实际获得的奖励与预测价值之间的差异。

5. **更新 \(\theta\)（在策略网络中）使用策略梯度**：
   - 使用策略梯度方法更新策略网络的参数 \(\theta\)，以优化策略。

对价值网络的更新可以看我的价值学习的文章。
对策略网络的更新可以看我的策略学习的文章。