
# 常见的激活函数

## 1.为什么需要激活函数

### 1.1 引入非线性因素

**线性模型的局限性** ：

如果没有激活函数，**神经网络的每一层仅仅是输入的线性组合**。即使有多层，整个网络仍然是一个线性模型。**线性模型只能学习输入和输出之间的线性关系**，而现实世界中的许多问题（如图像识别、语音识别、自然语言处理等）往往具有复杂的非线性关系。例如，**一张图片中的像素值与图像的类别之间并不是简单的线性关系**。就好像一个一次函数无法你和二次函数一样

**非线性激活函数的作用**：

激活函数是非线性函数，它能够将神经元的输出从**线性空间映射到非线性空间**。通过引入非线性因素，神经网络可以学习和模拟复杂的非线性关系，**从而更好地处理各种复杂的任务**。

线性模型只能学习输入和输出之间的**线性关系**，这是由其**数学形式和结构**决定的。线性模型的输出是输入特征的线性组合，这种形式决定了它无法捕捉输入特征之间的复杂非线性关系。为了处理现实世界中的复杂问题，需要使用非线性模型，通过引入非线性激活函数或高维映射，**非线性模型可以学习输入特征之间的复杂交互关系，从而更好地拟合数据。**

### 1.2 解决梯度消失和梯度爆炸问题

#### 1.2.1**梯度消失问题**

在训练深度神经网络时，如果使用像 Sigmoid 或 Tanh 这样的激活函数，当输入的绝对值较大时，这些函数的梯度会接近于零。这会导致反向传播过程中梯度逐渐减小，甚至趋近于零，使得网络的权重更新非常缓慢，训练过程变得非常困难。这种现象称为梯度消失问题。

- **激活函数的饱和区域**  
  某些激活函数（如 Sigmoid 和 Tanh）在输入值的绝对值较大时，其输出会趋于饱和，即输出值接近其最大值或最小值。在这种情况下，激活函数的导数（即梯度）会变得非常小。例如：

  - **Sigmoid 函数**：当输入 $x$ 的绝对值较大时，$\sigma(x)$ 接近 0 或 1，其导数 $\sigma'(x) = \sigma(x)(1 - \sigma(x))$ 会接近 0。
  - **Tanh 函数**：当输入 $x$ 的绝对值较大时，$\tanh(x)$ 接近 -1 或 1，其导数 $\tanh'(x) = 1 - \tanh^2(x)$ 也会接近 0。
- **反向传播中的梯度乘积**  
  在反向传播过程中，梯度是通过链式法则逐层计算的。对于一个深度为 $L$ 的神经网络，第 $l$ 层的梯度 $\frac{\partial L}{\partial W_l}$ 可以表示为：
  $$
  \frac{\partial L}{\partial W_l} = \frac{\partial L}{\partial a_L} \cdot \frac{\partial a_L}{\partial a_{L-1}} \cdot \ldots \cdot \frac{\partial a_{l+1}}{\partial a_l} \cdot \frac{\partial a_l}{\partial W_l}
  $$
  其中，$\frac{\partial a_{i+1}}{\partial a_i}$ 是激活函数的导数。如果每一层的激活函数导数都很小（如在饱和区域），那么这些导数的乘积会变得非常小，导致梯度逐渐趋近于零。这种现象称为梯度消失问题。

**对训练的影响**：

- **权重更新缓慢**  
  当梯度趋近于零时，权重的更新量也会变得非常小。这使得网络的训练过程变得非常缓慢，甚至停滞不前。
- **难以训练深层网络**  
  梯度消失问题尤其在深层神经网络中更为严重。由于深层网络的梯度需要经过更多层的乘积，因此更容易出现梯度消失的情况。这使得训练深层网络变得非常困难。

#### 1.2.2**梯度爆炸问题**

相反，如果某些层的梯度过大，可能会导致权重更新过大，使得网络的训练变得不稳定。这种现象称为梯度爆炸问题。

- **激活函数的导数过大**  
  与梯度消失问题相反，某些激活函数的导数可能在某些输入值下变得非常大。例如，如果激活函数是线性的（如 $f(x) = x$），其导数为 1。在这种情况下，如果网络的权重较大，反向传播中的梯度可能会迅速增大。
- **权重初始化不当**  
  如果权重初始化值过大，那么在前向传播过程中，每一层的输出可能会迅速增大。在反向传播过程中，梯度也会相应地增大。例如，假设权重 $W$ 的初始值较大，那么在前向传播中，每一层的输出 $a = Wx$ 会迅速增大。在反向传播中，梯度 $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial a} \cdot x$ 也会相应地增大。
- **反向传播中的梯度乘积**  
  在反向传播过程中，梯度是通过链式法则逐层计算的。如果每一层的激活函数导数较大，或者权重初始化值较大，那么这些导数和权重的乘积会迅速增大，导致梯度爆炸。
**激活函数的作用**：
一些激活函数（如 ReLU 及其变体）通过设计可以缓解梯度消失和梯度爆炸问题。例如，**ReLU 在输入为正时梯度恒为 1**，不存在梯度消失问题；而 Leaky ReLU 和 PReLU 等变体则进一步解决了 ReLU 在负区间梯度为零的问题。

#### 1.2.3**影响**

- **权重更新过大**  
  当梯度过大时，权重的更新量也会变得非常大。这可能会导致权重的值迅速增大或减小，使得网络的训练过程变得不稳定。
- **训练过程不稳定**  
  梯度爆炸问题会导致网络的训练过程变得不稳定，甚至无法收敛。在极端情况下，权重的值可能会变得非常大或非常小，导致数值计算问题（如溢出或下溢）。

梯度消失和梯度爆炸是深度神经网络训练中常见的问题，它们都与神经网络的反向传播算法有关。以下是详细解释这两种问题的成因：

**解决方法**：

使用合适的权重初始化方法（如 Xavier 初始化或 He 初始化）可以缓解梯度爆炸问题。这些初始化方法可以确保每一层的输出和梯度的方差保持在一个合理的范围内。

- **梯度裁剪（Gradient Clipping）**  
  梯度裁剪是一种常用的解决梯度爆炸问题的方法。在每次更新权重之前，将梯度的值限制在一个合理的范围内。例如，可以将梯度的范数限制在一个阈值 $C$ 以内：
  $$
  \text{if } \|g\| > C, \quad g = \frac{C}{\|g\|} \cdot g
  $$
  其中，$g$ 是梯度，$\|g\|$ 是梯度的范数，$C$ 是阈值。
- **批量归一化（Batch Normalization）**  
  批量归一化可以对每一层的输入进行归一化，使得输入的分布更加稳定，从而缓解梯度爆炸问题。
- **门控机制（Gating Mechanisms）**  
  在循环神经网络（RNN）中，门控机制（如 LSTM 和 GRU）可以有效地缓解梯度爆炸问题。门控机制通过引入门控单元，控制信息的流动，从而避免梯度爆炸。

### 1.3 引入零中心化

零中心化是指激活函数的输出不是以零为中心分布的特性。换句话说，激活函数的输出均值不为零

在训练神经网络时，如果神经元的输出不是零中心化的，可能会导致训练过程变得缓慢。由于输出不是零中心化的，梯度更新可能会受到输入分布的影响，导致更新不均衡非零中心化的输出会导致梯度更新的方向和大小受到输入数据分布的影响，从而降低训练效率

一些激活函数（如 Tanh 和 ELU）的输出是零中心化的，或者接近零中心化。零中心化的输出可以使得梯度更新更加稳定，从而加速训练过程

### 1.4 控制神经元的输出范围

神经元的输出范围对其后续层的输入有重要影响。如果输出范围过大或过小，可能会导致后续层的梯度计算不稳定，或者使得网络的输出难以解释。

激活函数可以将神经元的输出限制在一定的范围内。例如，Sigmoid 函数将输出限制在 (0, 1) 之间，Tanh 函数将输出限制在 (-1, 1) 之间。这种输出范围的限制有助于稳定网络的训练，并且使得网络的输出更容易解释。

### 1.5引入稀疏性

在某些任务中，稀疏的神经元输出可以提高模型的效率和可解释性。稀疏性意味着只有部分神经元被激活，而其他神经元的输出为零。稀疏的神经网络可以减少计算量，并且更容易解释。

一些激活函数（如 ReLU）可以自然地引入稀疏性。ReLU 在输入小于零时输出为零，这使得网络中只有部分神经元被激活。这种稀疏性可以提高模型的效率，并且有助于减少过拟合。

### 1.6加速收敛

在训练深度神经网络时，收敛速度是一个关键因素。快速收敛可以减少训练时间，提高模型的实用性。一些激活函数（如 SELU）通过设计可以加速网络的收敛。SELU 通过特定的参数选择，使得网络在训练过程中自动归一化，从而加速训练过程。

### 1.7 增强模型的表达能力

神经网络的表达能力决定了它能够学习到的复杂模式。更强的表达能力意味着网络可以更好地拟合训练数据，并且在测试数据上具有更好的泛化能力。
不同的激活函数具有不同的非线性特性，选择合适的激活函数可以增强模型的表达能力。例如，Swish 和 Mish 等新型激活函数在某些任务中表现优于传统的 ReLU 和 Sigmoid，因为它们具有更好的非线性表达能力和平滑特性。

## 2. 常见的激活函数

激活函数在深度学习中起着至关重要的作用，它们为神经网络引入非线性因素，使得网络能够学习复杂的模式和关系。以下是一些常见的激活函数及其特点、公式和应用场景：

### 2.1 **Sigmoid 函数**

- **公式**：
  $$
  \sigma(x) = \frac{1}{1 + e^{-x}}
  $$
- **特点**：
  - 输出范围在 (0, 1) 之间，可以将神经元的输出映射到一个概率值。
  - 函数在两端趋于平缓，中间部分较为陡峭。
- **应用场景**：
  - 常用于二分类问题的输出层，因为其输出可以表示为概率。
- **缺点**：
  - 当输入的绝对值较大时，梯度接近于零，容易导致梯度消失问题。
  - 计算涉及指数运算，计算成本较高。

### 2.2 **Tanh 函数**

- **公式**：
  $$
  \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
  $$
- **特点**：
  - 输出范围在 (-1, 1) 之间，是中心化为零的激活函数。
  - 与 Sigmoid 类似，但其输出范围更宽，且在零点附近梯度更大。
- **应用场景**：
  - 常用于隐藏层，尤其是在循环神经网络（RNN）中。
- **缺点**：
  - 同样存在梯度消失问题，当输入的绝对值较大时，梯度会变得非常小。

### 2.3 **ReLU 函数（Rectified Linear Unit）**

- **公式**：
  $$
  f(x) = \max(0, x)
  $$
- **特点**：
  - 当输入大于零时，输出等于输入；当输入小于零时，输出为零。
  - 计算简单，仅涉及阈值操作。
  - 在输入为正时，梯度恒为 1，不存在梯度消失问题。
- **应用场景**：
  - 是目前最常用的激活函数之一，广泛应用于卷积神经网络（CNN）和深度神经网络（DNN）的隐藏层。
- **缺点**：
  - 当输入小于零时，梯度为零，可能导致部分神经元“死亡”，即永远输出为零。
  - 输出是非零中心化的，可能会影响训练速度。

### 2.4 **Leaky ReLU 函数**

- **公式**：
  $$
  f(x) = \begin{cases}
  x, & \text{if } x \geq 0 \\
  \alpha x, & \text{if } x < 0
  \end{cases}
  $$
  其中 $\alpha$ 是一个小的正数（如 0.01）。
- **特点**：
  - 解决了 ReLU 在负区间梯度为零的问题，当输入小于零时，仍有一定的梯度。
  - 参数 $\alpha$ 可以调节负区间梯度的大小。
- **应用场景**：
  - 在一些需要避免神经元死亡的场景中使用，尤其适用于训练较深的网络。
- **缺点**：
  - 参数 $\alpha$ 需要手动调整，且不同的 $\alpha$ 值可能对性能影响较大。

### 2.5 **PReLU 函数（Parametric ReLU）**

- **公式**：
  $$
  f(x) = \begin{cases}
  x, & \text{if } x \geq 0 \\
  \alpha x, & \text{if } x < 0
  \end{cases}
  $$
  其中 $\alpha$ 是一个可学习的参数。
- **特点**：
  - 与 Leaky ReLU 类似，但 $\alpha$ 是通过训练自动学习的，更具灵活性。
- **应用场景**：
  - 在一些需要自动调整负区间梯度的场景中使用。
- **缺点**：
  - 增加了训练的复杂性，因为需要学习额外的参数。

### 2.6 **ELU 函数（Exponential Linear Unit）**

- **公式**：
  $$
  f(x) = \begin{cases}
  x, & \text{if } x \geq 0 \\
  \alpha (e^{x} - 1), & \text{if } x < 0
  \end{cases}
  $$
  其中 $\alpha$ 是一个超参数。
- **特点**：
  - 在负区间引入了指数函数，使得负值部分的输出接近零，有助于缓解梯度消失问题。
  - 输出接近零中心化。
- **应用场景**：
  - 在一些需要缓解梯度消失问题的场景中使用。
- **缺点**：
  - 计算复杂度较高，因为涉及指数运算。

### 2.7 **SELU 函数（Scaled Exponential Linear Unit）**

- **公式**：
  $$
  f(x) = \lambda \begin{cases}
  x, & \text{if } x \geq 0 \\
  \alpha (e^{x} - 1), & \text{if } x < 0
  \end{cases}
  $$
  其中 $\lambda$ 和 $\alpha$ 是固定的常数。
- **特点**：
  - 通过特定的 $\lambda$ 和 $\alpha$ 值，保证网络在训练过程中自动归一化，有助于加速训练。
- **应用场景**：
  - 在一些需要快速收敛的场景中使用。
- **缺点**：
  - 对网络结构和参数有一定要求，使用不当可能导致性能下降。

### 2.8 **Softmax 函数**

- **公式**：
  $$
  \sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
  $$
  其中 $z$ 是输入向量，$K$ 是类别数。
- **特点**：
  - 输出是一个概率分布，所有输出值的和为 1。
  - 适用于多分类问题的输出层。
- **应用场景**：
  - 常用于多分类问题，如图像分类、文本分类等。
- **缺点**：
  - 计算复杂度较高，因为涉及指数运算和归一化。

### 2.9 **Softplus 函数**

- **公式**：
  $$
  f(x) = \ln(1 + e^{x})
  $$
- **特点**：
  - 是 ReLU 的平滑版本，输出始终为正。
  - 梯度在所有输入值上都不为零。
- **应用场景**：
  - 在一些需要平滑激活函数的场景中使用。
- **缺点**：
  - 计算复杂度较高，因为涉及对数和指数运算。

### 2.10 **Swish 函数**

- **公式**：
  $$
  f(x) = x \cdot \sigma(x)
  $$
  其中 $\sigma(x)$ 是 Sigmoid 函数。
- **特点**：
  - 是一个自门控激活函数，结合了线性和非线性的特性。
  - 在某些任务中表现优于 ReLU。
- **应用场景**：
  - 在一些需要更好的非线性表达能力的场景中使用。
- **缺点**：
  - 计算复杂度较高，因为涉及 Sigmoid 函数的计算。

### 2.11 **Mish 函数**

- **公式**：
  $$
  f(x) = x \cdot \tanh(\text{softplus}(x))
  $$
- **特点**：
  - 是一个平滑的、非单调的激活函数，结合了 ReLU 和 Swish 的优点。
  - 在某些任务中表现优于 ReLU 和 Swish。
- **应用场景**：
  - 在一些需要更好的非线性表达能力和平滑特性的场景中使用。
- **缺点**：
  - 计算复杂度较高，因为涉及多个复杂函数的组合。

### 总结

不同的激活函数适用于不同的场景，选择合适的激活函数可以显著提高神经网络的性能。ReLU 及其变体（如 Leaky ReLU、PReLU）因其计算效率高和缓解梯度消失问题而被广泛使用，而 Sigmoid 和 Tanh 则更多用于特定的输出层或需要概率输出的场景。Softmax 是多分类问题的标准选择，而一些新型激活函数（如 Swish、Mish）在某些任务中也表现出良好的性能。
