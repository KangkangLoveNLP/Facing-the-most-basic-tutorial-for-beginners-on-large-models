# RLHF 基于人类反馈进行的强化学习 Reinforcement Learning from Human Feedback

## 背景和动机

传统的强化学习依赖环境给出的奖励信号，但在复杂任务或者需要人类主观判断的任务，难以设计一个有效和准确的奖励信号是非常困难的，在自然语言处理（NLP）任务中，如何定义一个奖励函数来衡量生成文本的质量或与人类偏好的一致性是一个挑战。
RLHF的核心思想是引入人类的反馈来指导智能体的学习过程。人类可以通过标注、评分、偏好比较等方式提供反馈，这些反馈被转化为奖励信号，从而指导智能体学习更符合人类期望的行为。这种方法特别适用于那些奖励信号难以设计或主观性强的任务。

从公共互联网采集数据训练的LLM会反映互联网的语调，它可能会生成有害、无意义、虚假的信息，我们需要对齐大模型的输出和人类偏好和价值，所以采用RLHF来对齐大模型和人类偏好显得特别重要。

### 基本概念

- Actor model:是核心的生成模型，负责根据输入生成具体的输出（例如文本、动作等）。它的目标是学习如何根据输入生成最优的输出。就是我们要优化的 LLM。
- Critic model:Reward Model 是一个评估模型，用于为 Actor Model 的输出分配奖励值。它的目标是根据人类标注的偏好和质量标准，判断输出的好坏。负责计算Actor_model的状态动作值矩阵，也就是上面提到的Q函数（Reward模型只负责给最后一个token打分，给之前token打分的重任靠Critic_model 完成）。
- Reference model:不参与训练，基准模型，是Actor model的一个复制。让训练好的模型和之前的模型差异不能太大
- Reward model:Reward_model 的一个复制，负责给 LLM 生成的句子打分。

### 强化学习和NLP的交叉点

- 大模型生成完整answer的过程，视为PPO的一次完整的交互，reward_model的打分便是这次交互的reward；
- 大模型每生成一个token，视为PPO中的一步；

### 训练一个

