{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d73c92f",
   "metadata": {},
   "source": [
    "# 大模型的训练流程： PreTrain  --> SFT --> Reward Model --> PPO\n",
    "PPO  算法需要一个奖励模型来为模型的输出进行打分提供奖励\n",
    "首先需要收集数据，数据是一个三元组偏好数据{question, chosen, rejected}，其中question是问题，chosen是选择的答案（好的回答），rejected是拒绝的答案（不好的回答）。\n",
    "奖励模型指导大模型的训练，因此奖励模型的能力要和训练的模型能力一致（因为评价比生成要求低）或者更好\n",
    "\n",
    "## 损失函数 \n",
    "\n",
    "### 公式解释\n",
    "\n",
    "Loss = - logsigmoid(chosen - rejected)\n",
    "Loss = - log(1 / (1 + e^(-x)))\n",
    "\n",
    "1. **LogSigmoid 函数**:\n",
    "   - `logsigmoid(x)` 是对 `sigmoid(x)` 函数取对数。`sigmoid(x)` 函数定义为 `1 / (1 + exp(-x))`，它将输入值 `x` 映射到 (0, 1) 区间，表示某个事件发生的概率。\n",
    "   - `logsigmoid(x)` 函数则将这个概率映射到实数范围，这在数学上更便于处理，尤其是在优化问题中。\n",
    "\n",
    "2. **损失函数**:\n",
    "   - `Loss = - logsigmoid(chosen - rejected)`：这个公式表示损失函数是 `chosen - rejected` 的 logsigmoid 函数的负值。\n",
    "   - 这里，`chosen` 和 `rejected` 代表两个动作的值，通常是策略网络输出的对数概率（logits）。`chosen` 是被选择的动作的值，而 `rejected` 是被拒绝的动作的值。\n",
    "\n",
    "3. **等价形式**:\n",
    "   - `Loss = - log(1 / (1 + e^(-x)))`，其中 `x = chosen - rejected`。\n",
    "   - 这个等价形式直接展示了 logsigmoid 函数的数学表达式，其中 `x` 是两个动作值的差。\n",
    "\n",
    "### 应用场景\n",
    "\n",
    "这种损失函数在强化学习中，特别是在策略梯度方法中，用于优化策略。它鼓励策略网络更倾向于选择那些能够带来更高回报的动作。具体来说：\n",
    "\n",
    "- 当 `chosen` 的值远大于 `rejected` 的值时，`chosen - rejected` 的值会很大，`logsigmoid(chosen - rejected)` 的值接近 0，因此损失函数的值会很小，这表示策略选择的动作是正确的。\n",
    "- 相反，如果 `chosen` 和 `rejected` 的值相近，或者 `chosen` 的值小于 `rejected` 的值，损失函数的值会较大，这表示策略需要调整，以更倾向于选择 `chosen` 动作。\n",
    "\n",
    "### 总结\n",
    "\n",
    "这个损失函数通过比较被选择的动作和被拒绝的动作的值，来指导策略网络的学习过程。它的目标是最小化损失，从而使得策略网络能够更倾向于选择那些能够带来更高回报的动作。这是强化学习中策略优化的一个关键步骤。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63667576",
   "metadata": {},
   "source": [
    "### 训练的一个奖励模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63c8d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "from trl import RewardTrainer, RewardConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e88bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'D:\\work\\models\\Meta-Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                           num_labels=1,# 分类为一时是一个回归模型\n",
    "                                                           quantization_config=bnb_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\",\n",
    "                    \"v_proj\",\n",
    "                    \"k_proj\",\n",
    "                    \"o_proj\",\n",
    "                    \"gate_proj\",\n",
    "                    \"down_proj\",\n",
    "                    \"up_proj\"\n",
    "                    ],\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "items = []\n",
    "with open(\"./data/preference.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        items.append(item)\n",
    "\n",
    "dataset = Dataset.from_list(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c37f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    chosen = example[\"question\"] + example[\"chosen\"]# 将问题和回答拼接在一起\n",
    "    rejected = example[\"question\"] + example[\"rejected\"]\n",
    "\n",
    "    tokenized_chosen = tokenizer(chosen)\n",
    "    tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "    new_example = {}\n",
    "    new_example[\"input_ids_chosen\"] = tokenized_chosen[\"input_ids\"]\n",
    "    new_example[\"attention_mask_chosen\"] = tokenized_chosen[\"attention_mask\"]\n",
    "    new_example[\"input_ids_rejected\"] = tokenized_rejected[\"input_ids\"]\n",
    "    new_example[\"attention_mask_rejected\"] = tokenized_rejected[\"attention_mask\"]\n",
    "    return new_example\n",
    "\n",
    "\n",
    "dataset = dataset.map(process_func, remove_columns=['question', 'chosen', 'rejected'])\n",
    "print(dataset)\n",
    "\n",
    "config = RewardConfig(output_dir=\"./reward_model\")\n",
    "config.num_train_epochs = 1\n",
    "config.per_device_train_batch_size = 1\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=config,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./reward_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
