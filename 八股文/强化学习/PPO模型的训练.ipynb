{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb5c4ca",
   "metadata": {},
   "source": [
    "# PPO 模型的训练\n",
    "\n",
    "## 我们需要的模型\n",
    "\n",
    "- 1. **基准模型**：一般是SFT后的模型作为基准，新训练的模型不能和这个模型的概率分布相差太大。\n",
    "- 2. **训练模型**： 他的结构和基准模型是一样的。\n",
    "- 3. **reward模型**：对一个问答序列进行打分，输出是一个分数。输出为hidden_size*1。\n",
    "- 4. **状态价值模型**：对每个状态进行评估，对截止到目前的序列预测到序列生成结束后这个序列的期望回报是多少，对每个token都输出分数，输出是一个分数。输出为hidden_size*1。\n",
    "\n",
    "我们可以使用LoRA技术，只使用一个大模型，多个LoRA层，来完成这个任务。减少训练时对显存的占用。训练模型和状态价值模型可以共用一个loRA层，不同的头来实现。\n",
    "\n",
    "## 实现流程伪代码\n",
    "\n",
    "```python\n",
    "for batch_prompt in prompt_dataset:\n",
    "    batch_response = active_model.generate(batch_prompt)# 策略模型的响应\n",
    "    batch_data = concat(batch_prompt, batch_response)# 连接问题和响应\n",
    "    batch_scores = reward_model(batch_data)# 计算得分\n",
    "\n",
    "    batch_all_probs, batch_probs, batch_all_values = active_model.forward_pass(batch_data)# 对批次数据进行前向传播，得到所有可能动作的概率（`batch_all_probs`）、选择动作的概（`batch_probs`）和所有可能动作的价值（`batch_all_values`）\n",
    "    ref_all_probs, ref_probs, ref_all_values = ref_model.forward_pass(batch_data)# 计算基础模型的所有可能动作的概率（`ref_all_probs`）、选择动作的概率（`ref_probs`）和所有可能动作的价值（`ref_all_values`）\n",
    "    kls = compute_KL(batch_all_probs, ref_all_probs)# 计算KL散度\n",
    "    rewards = compute_rewards(batch_scores, kls)# 根据得分和KL散度计算奖励。\n",
    "    advantages = compute_advantages(batch_all_values, rewards)# 计算优势函数，即奖励与价值函数估计之间的差异。\n",
    "    returns = advantages + batch_all_values# 计算回报，即优势函数与价值函数估计的和。\n",
    " \n",
    "   for i in range(epoch):\n",
    "       active_all_probs, active_probs, active_all_values = active_model.forward_pass(batch_data)\n",
    "\n",
    "       loss_state_value = torch.mean((returns - active_all_values) ** 2)\n",
    "       ratio = active_probs / batch_probs\n",
    "       loss_ppo = torch.mean(-advantages * ratio)\n",
    "       loss = loss_ppo + value_loss_rate * loss_state_value\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "\n",
    "$$ Loss_{PPO} = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_{\\theta''}^{GAE}(s_n^t, a_n^t) \\frac{P_{\\theta}(a_n^t | s_n^t)}{P_{\\theta''}(a_n^t | s_n^t)} $$\n",
    " 在提供的代码片段中，PPO（Proximal Policy Optimization）算法的损失函数体现在以下部分：\n",
    "\n",
    "```python\n",
    "ratio = active_probs / batch_probs\n",
    "loss_ppo = torch.mean(-advantages * ratio)\n",
    "```\n",
    "\n",
    "让我们详细解释这些代码行是如何与PPO算法的损失函数公式相对应的：\n",
    "\n",
    "### 公式解释\n",
    "\n",
    "PPO算法的损失函数公式为：\n",
    "\n",
    "$$ Loss_{PPO} = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} A_{\\theta''}^{GAE}(s_n^t, a_n^t) \\frac{P_{\\theta}(a_n^t | s_n^t)}{P_{\\theta''}(a_n^t | s_n^t)} $$\n",
    "其中：\n",
    "- $N$ 是批次大小。\n",
    "- $T_n$ 是每个样本的时间步数。\n",
    "- $A_{\\theta''}^{GAE}(s_n^t, a_n^t)$ 是优势函数，使用GAE（Generalized Advantage Estimation）计算。\n",
    "- $P_{\\theta}(a_n^t | s_n^t)$ 是新策略在状态 $s_n^t$ 下选择动作 $a_n^t$ 的概率。\n",
    "- $P_{\\theta''}(a_n^t | s_n^t)$ 是旧策略在状态 $s_n^t$ 下选择动作 $a_n^t$ 的概率。\n",
    "\n",
    "### 代码解释\n",
    "\n",
    "1. **计算概率比率**：\n",
    "   ```python\n",
    "   ratio = active_probs / batch_probs\n",
    "   ```\n",
    "   - `active_probs` 是新策略在给定状态下选择动作的概率。\n",
    "   - `batch_probs` 是基准模型在给定状态下选择动作的概率。\n",
    "   - 这对应于公式中的 $\\frac{P_{\\theta}(a_n^t | s_n^t)}{P_{\\theta''}(a_n^t | s_n^t)}$。\n",
    "\n",
    "2. **计算PPO损失**：\n",
    "   ```python\n",
    "   loss_ppo = torch.mean(-advantages * ratio)\n",
    "   ```\n",
    "   - `advantages` 是优势函数的估计值，对应于公式中的 $A_{\\theta''}^{GAE}(s_n^t, a_n^t)$。\n",
    "   - `-advantages * ratio` 计算了PPO损失的一部分，对应于公式中的 $-A_{\\theta''}^{GAE}(s_n^t, a_n^t) \\frac{P_{\\theta}(a_n^t | s_n^t)}{P_{\\theta''}(a_n^t | s_n^t)}$。\n",
    "   - `torch.mean()` 计算了所有样本的平均值，对应于公式中的 $\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n}$。\n",
    "\n",
    "3. **计算总损失**：\n",
    "   ```python\n",
    "   loss = loss_ppo + value_loss_rate * loss_state_value\n",
    "   ```\n",
    "   - `loss_state_value` 是状态价值损失，用于衡量价值函数的估计值与实际回报之间的差异。\n",
    "   - `value_loss_rate` 是状态价值损失的权重。\n",
    "   - 这对应于公式中的 $\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n}$ 部分，但公式中没有直接体现状态价值损失。\n",
    "\n",
    "总结来说，代码中的 `loss_ppo` 计算部分直接体现了PPO算法损失函数的核心思想，即通过计算新旧策略概率比率与优势函数的乘积来优化策略。而状态价值损失部分则是为了提高价值函数的估计精度。\n",
    " batch训练为外循环训练，训练epoch为内循环训练。每次用当前训练的模型作为重要性采样的模型计算advantage，训练epoch次模型\n",
    "\n",
    "### 数据准备阶段\n",
    "\n",
    "1. **遍历数据集**：\n",
    "   - `for batch_prompt in prompt_dataset:`：遍历数据集中的每个批次的提示（prompt）。\n",
    "\n",
    "2. **生成响应**：\n",
    "   - `batch_response = active_model.generate(batch_prompt)`：使用当前的策略模型（`active_model`）根据提示生成响应。\n",
    "\n",
    "3. **合并数据**：\n",
    "   - `batch_data = concat(batch_prompt, batch_response)`：将提示和响应合并成一个批次的数据。\n",
    "\n",
    "4. **计算奖励**：\n",
    "   - `batch_scores = reward_model(batch_data)`：使用奖励模型（`reward_model`）计算批次数据的得分，这些得分将用于计算奖励。\n",
    "\n",
    "5. **前向传播**：\n",
    "   - `batch_all_probs, batch_probs, batch_all_values = active_model.forward_pass(batch_data)`：对批次数据进行前向传播，得到所有可能动作的概率（`batch_all_probs`）、选择动作的概率（`batch_probs`）和所有可能动作的价值（`batch_all_values`）。\n",
    "   - `ref_all_probs, ref_probs, ref_all_values = ref_model.forward_pass(batch_data)`：对批次数据进行前向传播，得到参考模型（`ref_model`）的所有可能动作的概率（`ref_all_probs`）、选择动作的概率（`ref_probs`）和所有可能动作的价值（`ref_all_values`）。\n",
    "\n",
    "6. **计算KL散度**：\n",
    "   - `kls = compute_KL(batch_all_probs, ref_all_probs)`：计算当前策略模型和参考模型之间的KL散度，用于衡量两个概率分布的差异。\n",
    "\n",
    "7. **计算奖励**：\n",
    "   - `rewards = compute_rewards(batch_scores, kls)`：根据得分和KL散度计算奖励。\n",
    "\n",
    "8. **计算优势**：\n",
    "   - `advantages = compute_advantages(batch_all_values, rewards)`：计算优势函数，即奖励与价值函数估计之间的差异。\n",
    "\n",
    "9. **计算回报**：\n",
    "   - `returns = advantages + batch_all_values`：计算回报，即优势函数与价值函数估计的和。\n",
    "\n",
    "### 训练阶段\n",
    "\n",
    "1. **遍历训练周期**：\n",
    "   - `for i in range(epoch):`：遍历每个训练周期。\n",
    "\n",
    "2. **前向传播**：\n",
    "   - `active_all_probs, active_probs, active_all_values = active_model.forward_pass(batch_data)`：再次对批次数据进行前向传播，得到当前策略模型的概率和价值。\n",
    "\n",
    "3. **计算状态价值损失**：\n",
    "   - `loss_state_value = torch.mean((returns - active_all_values) ** 2)`：计算状态价值损失，即回报与价值函数估计之间的均方误差。\n",
    "\n",
    "4. **计算概率比率**：\n",
    "   - `ratio = active_probs / batch_probs`：计算新旧策略选择动作的概率比率。\n",
    "\n",
    "5. **计算PPO损失**：\n",
    "   - `loss_ppo = torch.mean(-advantages * ratio)`：计算PPO损失，即优势函数与概率比率的乘积的负均值。\n",
    "\n",
    "6. **计算总损失**：\n",
    "   - `loss = loss_ppo + value_loss_rate * loss_state_value`：计算总损失，即PPO损失与状态价值损失的加权和。\n",
    "\n",
    "7. **反向传播**：\n",
    "   - `loss.backward()`：对总损失进行反向传播，计算梯度。\n",
    "\n",
    "8. **更新模型参数**：\n",
    "   - `optimizer.step()`：使用优化器更新模型参数。\n",
    "\n",
    "9. **清零梯度**：\n",
    "   - `optimizer.zero_grad()`：清零梯度，为下一次迭代做准备。\n",
    "\n",
    "这段代码实现了PPO算法的核心思想，即通过限制策略更新的幅度来提高策略的稳定性，并通过优势函数和价值函数的估计来优化策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667dd52",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "# 训练数据只需要query即可\n",
    "model_path = r'D:\\work\\models\\Meta-Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\",\n",
    "                    \"v_proj\",\n",
    "                    \"k_proj\",\n",
    "                    \"o_proj\",\n",
    "                    \"gate_proj\",\n",
    "                    \"down_proj\",\n",
    "                    \"up_proj\"\n",
    "                    ],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_path,\n",
    "                                                          reward_adapter=\"./reward_model\",\n",
    "                                                          peft_config=peft_config,\n",
    "                                                          quantization_config=bnb_config\n",
    "                                                          )\n",
    "model.to(\"cuda\")\n",
    "\n",
    "items = []\n",
    "with open(\"./data/queries.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        items.append(json.loads(line))\n",
    "queries_dataset = Dataset.from_list(items)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    queries = []\n",
    "    for item in data:\n",
    "        queries.append(tokenizer(item[\"query\"], return_tensors=\"pt\")[\"input_ids\"].squeeze().to(\"cuda\"))\n",
    "    return queries\n",
    "\n",
    "\n",
    "ppo_config = PPOConfig(kl_penalty=\"full\", ppo_epochs=3, batch_size=2, mini_batch_size=1)\n",
    "ppo_trainer = PPOTrainer(config=ppo_config, model=model, ref_model=None, tokenizer=tokenizer, dataset=queries_dataset,\n",
    "                         data_collator=collator)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "}\n",
    "\n",
    "for batch in ppo_trainer.dataloader:\n",
    "    query_tensors = batch\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        query_tensors, return_prompt=False,  **generation_kwargs)\n",
    "    scores = []\n",
    "    for query, response in zip(query_tensors, response_tensors):\n",
    "        input_ids = torch.concat([query, response], dim=0)\n",
    "        input_ids = torch.unsqueeze(input_ids, dim=0)\n",
    "        score = ppo_trainer.model.compute_reward_score(input_ids=input_ids)[0, -1, 0]\n",
    "        scores.append(score)\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, scores)\n",
    "ppo_trainer.save_pretrained(\"./rl_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
