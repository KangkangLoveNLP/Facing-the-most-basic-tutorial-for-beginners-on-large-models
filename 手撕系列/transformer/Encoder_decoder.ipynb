{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在这个任务中，你将构建 Transformer 架构中的编码器部分。\n",
    "- 将使用 PyTorch 框架实现以下组件\n",
    "  - 多头注意力（MHA）模块\n",
    "  - 位置感知前馈神经网络\n",
    "  - 输出层接收编码器输出并预测 token_ids。\n",
    "  - (可选地)研究添加位置信息是否有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=6\n",
    "d_model = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.zeros(max_len, d_model)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.0100])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0100],\n",
       "        [ 0.9093,  0.0200],\n",
       "        [ 0.1411,  0.0300],\n",
       "        [-0.7568,  0.0400],\n",
       "        [-0.9589,  0.0500]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:, 0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0000,  0.0100,  0.0000],\n",
       "        [ 0.9093,  0.0000,  0.0200,  0.0000],\n",
       "        [ 0.1411,  0.0000,  0.0300,  0.0000],\n",
       "        [-0.7568,  0.0000,  0.0400,  0.0000],\n",
       "        [-0.9589,  0.0000,  0.0500,  0.0000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
       "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
       "        [-0.9589,  0.2837,  0.0500,  0.9988]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  1.0000,  0.0000,  1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8415,  0.5403,  0.0100,  0.9999]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9093, -0.4161,  0.0200,  0.9998]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1411, -0.9900,  0.0300,  0.9996]]],\n",
       "\n",
       "\n",
       "        [[[-0.7568, -0.6536,  0.0400,  0.9992]]],\n",
       "\n",
       "\n",
       "        [[[-0.9589,  0.2837,  0.0500,  0.9988]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right).\n",
    "$$\n",
    "\n",
    "其中，pos表示位置，d_model表示模型的维度，一般设置为512。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PositionalEncoding` 类是 Transformer 模型中一个非常重要的组件，它的作用是为输入的嵌入向量（embedding）添加位置信息。Transformer 是一个基于序列的模型，但它本身并不像 RNN 那样能够自然地处理序列中的位置信息。因此，需要通过某种方式将位置信息注入到模型中，`PositionalEncoding` 就是实现这一功能的关键部分。\n",
    "\n",
    "### 代码解析\n",
    "\n",
    "#### 1. 初始化方法 `__init__`\n",
    "```python\n",
    "def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)\n",
    "```\n",
    "\n",
    "- **`d_model`**：表示嵌入向量的维度，即每个位置编码的大小。\n",
    "- **`dropout`**：用于正则化，防止过拟合。\n",
    "- **`max_len`**：表示模型能够处理的最大序列长度。\n",
    "\n",
    "#### 生成位置编码\n",
    "位置编码的生成基于论文《Attention Is All You Need》中提出的方法。具体来说，位置编码是一个固定大小的向量（维度为 `d_model`），它通过正弦和余弦函数生成。这种编码方式能够捕捉到位置信息，并且可以处理比训练时序列长度更长的序列。\n",
    "\n",
    "- **`torch.zeros(max_len, d_model)`**：创建一个形状为 `(max_len, d_model)` 的零矩阵，用于存储位置编码。\n",
    "- **`torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)`**：生成一个从 0 到 `max_len-1` 的序列，并将其转换为浮点数，然后通过 `unsqueeze(1)` 将其扩展为二维张量，形状为 `(max_len, 1)`。这表示每个位置的索引。\n",
    "- **`torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))`**：计算分母部分，用于调整正弦和余弦函数的频率。`torch.arange(0, d_model, 2)` 表示每隔一个维度取一个值（因为 `d_model` 是偶数），`-math.log(10000.0) / d_model` 是一个缩放因子，用于控制频率的变化范围。\n",
    "- **`pe[:, 0::2] = torch.sin(position * div_term)`** 和 **`pe[:, 1::2] = torch.cos(position * div_term)`**：分别将正弦和余弦函数的值赋值到位置编码矩阵的偶数和奇数位置上。这样，每个位置编码的偶数维度由正弦函数生成，奇数维度由余弦函数生成。\n",
    "- **`pe = pe.unsqueeze(0).transpose(0, 1)`**：将位置编码矩阵的形状调整为 `(max_len, 1, d_model)`，然后转置为 `(1, max_len, d_model)`，以便在后续操作中可以直接与输入张量相加。\n",
    "- **`self.register_buffer('pe', pe)`**：将位置编码矩阵注册为一个缓冲区（buffer），这样它不会被视为模型的参数，但会在模型的 `state_dict` 中保存，并且会随着模型的移动（如从 CPU 移动到 GPU）而自动移动。\n",
    "在代码中，位置编码（Positional Encoding）的生成是基于论文《Attention Is All You Need》中提出的公式。具体公式如下：\n",
    "\n",
    "对于每个位置 \\( pos \\) 和每个维度 \\( i \\)，位置编码 \\( PE(pos, i) \\) 的计算公式为：\n",
    "\n",
    "\\[\n",
    "PE(pos, i) = \n",
    "\\begin{cases}  \n",
    "\\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) & \\text{if } i \\text{ is even} \\\\  \n",
    "\\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) & \\text{if } i \\text{ is odd}  \n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "- \\( pos \\) 是位置索引（从 0 开始）。\n",
    "- \\( i \\) 是维度索引（从 0 开始）。\n",
    "- \\( d_{model} \\) 是嵌入向量的维度。\n",
    "- \\( 10000 \\) 是一个常数，用于控制频率的变化范围。\n",
    "\n",
    "### 代码与公式对应关系\n",
    "\n",
    "在代码中，位置编码的生成过程与上述公式一一对应：\n",
    "\n",
    "```python\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "```\n",
    "\n",
    "1. **`position`**：\n",
    "   ```python\n",
    "   position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "   ```\n",
    "   这里生成了一个从 0 到 `max_len-1` 的序列，表示每个位置的索引 $pos$。`unsqueeze(1)` 将其扩展为二维张量，形状为 `(max_len, 1)`，方便后续的广播操作。\n",
    "\n",
    "2. **`div_term`**：\n",
    "   ```python\n",
    "   div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "   ```\n",
    "   这里计算了公式中的分母部分：\n",
    "   \\[\n",
    "   \\frac{1}{10000^{2i/d_{model}}}\n",
    "   \\]\n",
    "   其中：\n",
    "   - `torch.arange(0, d_model, 2)` 表示每隔一个维度取一个值（因为偶数维度用正弦，奇数维度用余弦）。\n",
    "   - `(-math.log(10000.0) / d_model)` 是公式中的指数部分。\n",
    "   - `torch.exp(...)` 计算 \\( e^{x} \\)，从而得到分母的值。\n",
    "\n",
    "3. **`pe[:, 0::2]` 和 `pe[:, 1::2]`**：\n",
    "   ```python\n",
    "   pe[:, 0::2] = torch.sin(position * div_term)\n",
    "   pe[:, 1::2] = torch.cos(position * div_term)\n",
    "   ```\n",
    "   这里分别计算了偶数维度和奇数维度的位置编码：\n",
    "   - `pe[:, 0::2]` 对应公式中的正弦部分：\n",
    "     $$\n",
    "     PE(pos, i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\quad \\text{(for even } i\\text{)}\n",
    "     $$\n",
    "   - `pe[:, 1::2]` 对应公式中的余弦部分：\n",
    "     $$\n",
    "     PE(pos, i) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\quad \\text{(for odd } i\\text{)}\n",
    "     $$\n",
    "### 1.3max_len的选择\n",
    "在代码中，`max_len` 表示模型能够处理的最大序列长度。这个值是预先设定的，用于定义位置编码矩阵的大小。在实际应用中，`max_len` 应该足够大，以覆盖你预期中可能遇到的最长序列。\n",
    "原始代码在实际输入时，只使用与输入序列长度相等的部分位置编码\n",
    "#### 1.3.1 代码中的 `max_len`\n",
    "\n",
    "```python\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "```\n",
    "\n",
    "这里，`pe` 是一个形状为 `(max_len, d_model)` 的零矩阵，用于存储位置编码。`max_len` 是这个矩阵的行数，表示可以处理的最大序列长度。\n",
    "\n",
    "#### 1.3.2选择 `max_len` 的值\n",
    "\n",
    "在实际应用中，选择 `max_len` 的值需要考虑以下因素：\n",
    "\n",
    "1. **数据集的特性**：如果你的数据集中序列的长度变化很大，那么 `max_len` 应该足够大，以覆盖最长的序列。\n",
    "2. **模型的容量**：较大的 `max_len` 会增加模型的参数量，因为位置编码矩阵的大小与 `max_len` 直接相关。这可能会增加模型的复杂度和训练时间。\n",
    "3. **计算资源**：较大的 `max_len` 会占用更多的内存和计算资源，特别是在使用 GPU 进行训练时。\n",
    "\n",
    "#### 1.3.3 实际应用\n",
    "\n",
    "在实际应用中，`max_len` 的值通常根据数据集的统计特性来确定。例如，如果数据集中 99% 的序列长度都小于 100，那么可以选择 `max_len = 100`。这样可以确保模型能够处理大多数序列，同时避免不必要的计算开销。\n",
    "\n",
    "#### 1.3.4总结\n",
    "\n",
    "`max_len` 是一个重要的超参数，它定义了模型能够处理的最大序列长度。选择合适的 `max_len` 值需要考虑数据集的特性、模型的容量和计算资源。在实际应用中，可以通过分析数据集的统计特性来确定 `max_len` 的值。\n",
    "\n",
    "#### 2. 前向传播方法 `forward`\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "```\n",
    "\n",
    "- **`x`**：输入的嵌入向量，形状为 `(seq_len, batch_size, d_model)`。\n",
    "- **`self.pe[:x.size(0), :]`**：根据输入序列的实际长度（`x.size(0)`），从预定义的位置编码矩阵中取出相应长度的位置编码。\n",
    "- **`x + self.pe[:x.size(0), :]`**：将位置编码加到输入的嵌入向量上。由于位置编码的形状为 `(seq_len, 1, d_model)`，而输入嵌入向量的形状为 `(seq_len, batch_size, d_model)`，PyTorch 会自动进行广播操作，将位置编码应用到每个样本上。\n",
    "- **`self.dropout(x)`**：对加了位置编码后的嵌入向量应用 Dropout，以防止过拟合。\n",
    "\n",
    "### 总结\n",
    "`PositionalEncoding` 类的作用是为输入的嵌入向量添加位置信息，使得 Transformer 模型能够感知序列中每个元素的位置。位置编码通过正弦和余弦函数生成，能够捕捉到位置信息，并且可以处理比训练时序列长度更长的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"嵌入尺寸需要被头部整除\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的！我们再详细解析一下 `MultiHeadAttention` 类的代码，这次我会结合具体的例子来说明每一步的作用和计算过程。\n",
    "\n",
    "### 1. 初始化方法 `__init__`\n",
    "\n",
    "```python\n",
    "def __init__(self, embed_size, heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.heads = heads\n",
    "    self.head_dim = embed_size // heads\n",
    "\n",
    "    assert self.head_dim * heads == embed_size, \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "    self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "    self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "```\n",
    "\n",
    "#### 参数解释\n",
    "- **`embed_size`**：嵌入向量的维度，表示每个输入向量的大小。\n",
    "- **`heads`**：注意力头的数量。多头注意力机制将输入分割成多个“头”，每个头学习不同的特征。\n",
    "- **`head_dim`**：每个注意力头的维度大小，计算公式为 `embed_size // heads`。这意味着每个头处理的特征子集的大小。\n",
    "\n",
    "#### 线性变换层\n",
    "- **`self.values`**、**`self.keys`**、**`self.queries`**：\n",
    "  - 这些是线性变换层，用于将输入的嵌入向量分别转换为值（Values）、键（Keys）和查询（Queries）。\n",
    "  - 每个线性层的输入和输出维度都是 `self.head_dim`，因为每个头处理的特征子集大小为 `self.head_dim`。\n",
    "  - 使用 `bias=False` 是为了简化计算，避免引入额外的偏置项。\n",
    "\n",
    "- **`self.fc_out`**：\n",
    "  - 在多头注意力计算完成后，将所有头的输出拼接起来，并通过一个线性层将维度转换回原始的嵌入维度 `embed_size`。\n",
    "\n",
    "### 2. 前向传播方法 `forward`\n",
    "\n",
    "```python\n",
    "def forward(self, values, keys, query, mask):\n",
    "    N = query.shape[0]  # Batch size\n",
    "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "```\n",
    "\n",
    "#### 输入参数\n",
    "- **`values`**、**`keys`**、**`query`**：\n",
    "  - 这三个输入张量的形状通常为 `(batch_size, seq_len, embed_size)`。\n",
    "  - 它们分别对应于值（Values）、键（Keys）和查询（Queries）。\n",
    "- **`mask`**：\n",
    "  - 用于遮蔽某些位置的注意力权重，避免模型关注到不应该关注的部分（例如，解码器中的未来信息）。\n",
    "\n",
    "#### 多头注意力计算过程\n",
    "\n",
    "1. **将输入嵌入分割为多个头**：\n",
    "   ```python\n",
    "   values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "   keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "   queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "   ```\n",
    "   - 将输入的嵌入向量分割成 `heads` 个头，每个头的维度为 `self.head_dim`。\n",
    "   - 例如，如果 `embed_size = 256`，`heads = 8`，则 `self.head_dim = 32`，每个头处理 32 维的特征。\n",
    "   - 重塑后的形状为 `(N, seq_len, heads, head_dim)`。\n",
    "\n",
    "2. **线性变换**：\n",
    "   ```python\n",
    "   values = self.values(values)\n",
    "   keys = self.keys(keys)\n",
    "   queries = self.queries(queries)\n",
    "   ```\n",
    "   - 对每个头的值、键和查询分别进行线性变换。\n",
    "   - 这一步将输入特征投影到不同的子空间中，使得每个头可以学习不同的特征。\n",
    "\n",
    "3. **计算注意力分数（Attention Scores）**：\n",
    "   ```python\n",
    "   energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "   ```\n",
    "   - 使用 `torch.einsum` 计算查询和键之间的点积，得到注意力分数矩阵。\n",
    "   - 公式 `nqhd,nkhd->nhqk` 表示：\n",
    "     - `n`：批量大小（Batch Size）。\n",
    "     - `q`：查询序列的长度。\n",
    "     - `k`：键序列的长度。\n",
    "     - `h`：头的数量。\n",
    "     - `d`：每个头的维度。\n",
    "   - 输出的 `energy` 形状为 `(N, heads, query_len, key_len)`。\n",
    "\n",
    "4. **应用掩码（Masking）**：\n",
    "   ```python\n",
    "   if mask is not None:\n",
    "       energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "   ```\n",
    "   - 如果提供了掩码，将掩码为 0 的位置的注意力分数设置为一个非常小的值（如 `-1e20`），这样在后续的 softmax 计算中，这些位置的权重会趋近于 0。\n",
    "\n",
    "5. **计算注意力权重**：\n",
    "   ```python\n",
    "   attention = torch.softmax(energy / (self.embed_size ** (0.5)), dim=3)\n",
    "   ```\n",
    "   - 对注意力分数进行 softmax 归一化，得到注意力权重。\n",
    "   - 除以 `sqrt(embed_size)` 是为了缩放点积结果，避免梯度消失或爆炸。\n",
    "\n",
    "6. **应用注意力权重**：\n",
    "   ```python\n",
    "   out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "       N, query_len, self.heads * self.head_dim\n",
    "   )\n",
    "   ```\n",
    "   - 使用 `torch.einsum` 将注意力权重与值相乘，得到加权的值。\n",
    "   - 公式 `nhql,nlhd->nqhd` 表示：\n",
    "     - `n`：批量大小。\n",
    "     - `h`：头的数量。\n",
    "     - `q`：查询序列的长度。\n",
    "     - `l`：值序列的长度。\n",
    "     - `d`：每个头的维度。\n",
    "   - 输出的 `out` 形状为 `(N, query_len, heads * self.head_dim)`。\n",
    "\n",
    "7. **线性变换输出**：\n",
    "   ```python\n",
    "   out = self.fc_out(out)\n",
    "   ```\n",
    "   - 将所有头的输出拼接起来，并通过一个线性层将维度转换回原始的嵌入维度 `embed_size`。\n",
    "\n",
    "### 3. 示例矩阵计算\n",
    "\n",
    "假设：\n",
    "- `embed_size = 4`\n",
    "- `heads = 2`\n",
    "- `head_dim = embed_size // heads = 2`\n",
    "- 输入序列长度为 3，批量大小为 1。\n",
    "\n",
    "#### 输入张量\n",
    "```python\n",
    "values = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]], dtype=torch.float32)\n",
    "keys = torch.tensor([[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]], dtype=torch.float32)\n",
    "query = torch.tensor([[[25, 26, 27, 28], [29, 30, 31, 32], [33, 34, 35, 36]]], dtype=torch.float32)\n",
    "mask = None\n",
    "```\n",
    "\n",
    "#### 重塑为多头\n",
    "```python\n",
    "values = values.reshape(1, 3, 2, 2)  # (N, value_len, heads, head_dim)\n",
    "keys = keys.reshape(1, 3, 2, 2)\n",
    "queries = query.reshape(1, 3, 2, 2)\n",
    "```\n",
    "\n",
    "#### 线性变换\n",
    "假设线性变换层的权重为单位矩阵（简化计算），则：\n",
    "```python\n",
    "values = self.values(values)  # 不改变值\n",
    "keys = self.keys(keys)\n",
    "queries = self.queries(queries)\n",
    "```\n",
    "\n",
    "#### 计算注意力分数\n",
    "```python\n",
    "energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "```\n",
    "\n",
    "假设：\n",
    "- `queries = [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]`\n",
    "- `keys = [[[13, 14], [15, 16]], [[17, 18], [19, 20]], [[21, 22], [23, 24]]]`\n",
    "\n",
    "计算点积：\n",
    "```python\n",
    "energy = [\n",
    "    [\n",
    "        [[1*13 + 2*14, 1*15 + 2*16], [1*17 + 2*18, 1*19 + 2*20]],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]], dtype=torch.float32)\n",
    "keys = torch.tensor([[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]], dtype=torch.float32)\n",
    "query = torch.tensor([[[25, 26, 27, 28], [29, 30, 31, 32], [33, 34, 35, 36]]], dtype=torch.float32)\n",
    "mask = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.]]]),\n",
       " tensor([[[13., 14., 15., 16.],\n",
       "          [17., 18., 19., 20.],\n",
       "          [21., 22., 23., 24.]]]),\n",
       " tensor([[[25., 26., 27., 28.],\n",
       "          [29., 30., 31., 32.],\n",
       "          [33., 34., 35., 36.]]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values,keys,query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = values.reshape(1, 3, 2, 2)  # (N, value_len, heads, head_dim)\n",
    "keys = keys.reshape(1, 3, 2, 2)\n",
    "queries = query.reshape(1, 3, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.,  2.],\n",
       "           [ 3.,  4.]],\n",
       " \n",
       "          [[ 5.,  6.],\n",
       "           [ 7.,  8.]],\n",
       " \n",
       "          [[ 9., 10.],\n",
       "           [11., 12.]]]]),\n",
       " tensor([[[[13., 14.],\n",
       "           [15., 16.]],\n",
       " \n",
       "          [[17., 18.],\n",
       "           [19., 20.]],\n",
       " \n",
       "          [[21., 22.],\n",
       "           [23., 24.]]]]),\n",
       " tensor([[[25., 26., 27., 28.],\n",
       "          [29., 30., 31., 32.],\n",
       "          [33., 34., 35., 36.]]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values,keys,query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 编码器\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码实现了一个Transformer编码器模块（Transformer Block），它是Transformer架构的核心组件之一。Transformer架构是一种基于自注意力机制（Self-Attention）的深度学习模型，广泛应用于自然语言处理（NLP）任务，如机器翻译、文本生成等。以下是对代码的详细解释：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **类定义**\n",
    "```python\n",
    "class TransformerBlock(nn.Module):\n",
    "```\n",
    "`TransformerBlock` 是一个继承自 PyTorch 的 `nn.Module` 的类，表示一个Transformer编码器模块。`nn.Module` 是 PyTorch 中所有神经网络模块的基类，用于定义和管理神经网络的结构。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **初始化方法**\n",
    "```python\n",
    "def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.attention = MultiHeadAttention(embed_size, heads)\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "        nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "```\n",
    "\n",
    "#### **参数解释**\n",
    "- `embed_size`: 嵌入向量的维度，表示每个词或标记（token）的特征维度。\n",
    "- `heads`: 多头注意力机制中的头数（`Multi-Head Attention`）。\n",
    "- `dropout`: Dropout比率，用于防止过拟合。\n",
    "- `forward_expansion`: 前馈网络（Feed-Forward Network, FFN）中隐藏层的扩展因子。\n",
    "\n",
    "#### **组件解释**\n",
    "1. **多头注意力机制（`MultiHeadAttention`）**\n",
    "   ```python\n",
    "   self.attention = MultiHeadAttention(embed_size, heads)\n",
    "   ```\n",
    "   这是Transformer的核心部分，实现了多头注意力机制。它允许模型在不同的表示子空间中学习信息。`MultiHeadAttention` 的具体实现没有在这段代码中给出，但通常它会将输入分为多个“头”，分别计算注意力权重，然后将结果拼接起来。\n",
    "\n",
    "2. **层归一化（`LayerNorm`）**\n",
    "   ```python\n",
    "   self.norm1 = nn.LayerNorm(embed_size)\n",
    "   self.norm2 = nn.LayerNorm(embed_size)\n",
    "   ```\n",
    "   层归一化（Layer Normalization）是一种归一化方法，用于稳定训练过程并加速收敛。它对每个样本的特征进行归一化，而不是像批量归一化（Batch Normalization）那样对整个批次进行归一化。\n",
    "\n",
    "3. **前馈网络（`Feed-Forward Network`）**\n",
    "   ```python\n",
    "   self.feed_forward = nn.Sequential(\n",
    "       nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "       nn.ReLU(),\n",
    "       nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "   )\n",
    "   ```\n",
    "   前馈网络是一个简单的两层全连接网络。它的作用是进一步处理多头注意力机制的输出。`forward_expansion` 参数控制隐藏层的大小，通常设置为一个较大的值（如4），表示隐藏层的维度是输入维度的4倍。\n",
    "\n",
    "4. **Dropout**\n",
    "   ```python\n",
    "   self.dropout = nn.Dropout(dropout)\n",
    "   ```\n",
    "   Dropout 是一种正则化技术，通过随机丢弃一部分神经元的输出来防止过拟合。`dropout` 参数表示丢弃的概率。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **前向传播方法**\n",
    "```python\n",
    "def forward(self, value, key, query, mask):\n",
    "    attention = self.attention(value, key, query, mask)\n",
    "\n",
    "    x = self.dropout(self.norm1(attention + query))\n",
    "    forward = self.feed_forward(x)\n",
    "    out = self.dropout(self.norm2(forward + x))\n",
    "    return out\n",
    "```\n",
    "\n",
    "#### **参数解释**\n",
    "- `value`: 值向量，用于计算注意力权重后的加权求和。\n",
    "- `key`: 键向量，用于计算注意力权重。\n",
    "- `query`: 查询向量，用于计算注意力权重。\n",
    "- `mask`: 掩码，用于防止某些位置的信息泄露（如在自注意力中屏蔽未来信息）。\n",
    "\n",
    "#### **流程解释**\n",
    "1. **多头注意力**\n",
    "   ```python\n",
    "   attention = self.attention(value, key, query, mask)\n",
    "   ```\n",
    "   首先，使用多头注意力机制计算注意力输出。`value`、`key` 和 `query` 是输入的三个部分，`mask` 用于控制哪些位置的信息可以被关注。\n",
    "\n",
    "2. **残差连接与层归一化**\n",
    "   ```python\n",
    "   x = self.dropout(self.norm1(attention + query))\n",
    "   ```\n",
    "   将注意力输出与输入的 `query` 进行残差连接（`attention + query`），然后通过层归一化（`LayerNorm`）。最后，应用 Dropout 防止过拟合。\n",
    "\n",
    "3. **前馈网络**\n",
    "   ```python\n",
    "   forward = self.feed_forward(x)\n",
    "   ```\n",
    "   将经过归一化的输出传递到前馈网络中进行进一步处理。\n",
    "\n",
    "4. **第二次残差连接与层归一化**\n",
    "   ```python\n",
    "   out = self.dropout(self.norm2(forward + x))\n",
    "   ```\n",
    "   将前馈网络的输出与之前的输出 `x` 进行残差连接，再次通过层归一化和 Dropout。\n",
    "\n",
    "5. **返回结果**\n",
    "   ```python\n",
    "   return out\n",
    "   ```\n",
    "   最终返回处理后的输出。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **总结**\n",
    "这段代码实现了一个标准的Transformer编码器模块，其核心包括：\n",
    "- 多头注意力机制（`MultiHeadAttention`）。\n",
    "- 残差连接（Residual Connection）。\n",
    "- 层归一化（`LayerNorm`）。\n",
    "- 前馈网络（`Feed-Forward Network`）。\n",
    "- Dropout 正则化。\n",
    "\n",
    "这些组件共同作用，使得Transformer能够高效地处理序列数据，并在许多NLP任务中取得了优异的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = PositionalEncoding(embed_size, dropout, max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "            N, seq_length = x.shape\n",
    "            x = self.dropout(self.position_embedding(self.word_embedding(x)))\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, mask)\n",
    "\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个完整的 **Transformer 编码器（Encoder）**，它是 Transformer 架构中的一个重要组成部分。编码器的作用是将输入序列（如源语言文本）转换为上下文表示，这些表示可以被解码器（Decoder）用于生成目标序列（如目标语言文本）。以下是对代码的详细解释：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **类定义**\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "```\n",
    "`Encoder` 是一个继承自 PyTorch 的 `nn.Module` 的类，用于定义 Transformer 编码器的结构。`nn.Module` 是 PyTorch 中所有神经网络模块的基类，用于定义和管理神经网络的结构。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **初始化方法**\n",
    "```python\n",
    "def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.device = device\n",
    "    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "    self.position_embedding = PositionalEncoding(embed_size, dropout, max_length)\n",
    "\n",
    "    self.layers = nn.ModuleList(\n",
    "        [\n",
    "            TransformerBlock(\n",
    "                embed_size,\n",
    "                heads,\n",
    "                dropout=dropout,\n",
    "                forward_expansion=forward_expansion,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "```\n",
    "\n",
    "#### **参数解释**\n",
    "- `src_vocab_size`: 源语言词汇表的大小，即输入序列中可能的标记（token）数量。\n",
    "- `embed_size`: 嵌入向量的维度，表示每个词或标记的特征维度。\n",
    "- `num_layers`: 编码器中 Transformer 块（`TransformerBlock`）的数量。\n",
    "- `heads`: 多头注意力机制中的头数。\n",
    "- `device`: 运行设备（如 CPU 或 GPU）。\n",
    "- `forward_expansion`: 前馈网络（FFN）中隐藏层的扩展因子。\n",
    "- `dropout`: Dropout 比率，用于防止过拟合。\n",
    "- `max_length`: 输入序列的最大长度，用于位置编码。\n",
    "\n",
    "#### **组件解释**\n",
    "1. **词嵌入（`word_embedding`）**\n",
    "   ```python\n",
    "   self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "   ```\n",
    "   词嵌入层将输入的标记（token）索引映射为固定维度的嵌入向量。`src_vocab_size` 是词汇表的大小，`embed_size` 是嵌入向量的维度。\n",
    "\n",
    "2. **位置编码（`position_embedding`）**\n",
    "   ```python\n",
    "   self.position_embedding = PositionalEncoding(embed_size, dropout, max_length)\n",
    "   ```\n",
    "   位置编码层用于为每个标记添加位置信息，使得模型能够捕捉序列中的顺序关系。`PositionalEncoding` 的具体实现没有在这段代码中给出，但通常它会根据标记的位置生成一个固定维度的向量，并将其与词嵌入相加。\n",
    "\n",
    "3. **Transformer 块（`TransformerBlock`）**\n",
    "   ```python\n",
    "   self.layers = nn.ModuleList(\n",
    "       [\n",
    "           TransformerBlock(\n",
    "               embed_size,\n",
    "               heads,\n",
    "               dropout=dropout,\n",
    "               forward_expansion=forward_expansion,\n",
    "           )\n",
    "           for _ in range(num_layers)\n",
    "       ]\n",
    "   )\n",
    "   ```\n",
    "   编码器由多个 Transformer 块组成。每个 Transformer 块包含多头注意力机制和前馈网络。`num_layers` 表示 Transformer 块的数量。\n",
    "\n",
    "4. **Dropout**\n",
    "   ```python\n",
    "   self.dropout = nn.Dropout(dropout)\n",
    "   ```\n",
    "   Dropout 是一种正则化技术，通过随机丢弃一部分神经元的输出来防止过拟合。`dropout` 参数表示丢弃的概率。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **前向传播方法**\n",
    "```python\n",
    "def forward(self, x, mask):\n",
    "    N, seq_length = x.shape\n",
    "    x = self.dropout(self.position_embedding(self.word_embedding(x)))\n",
    "\n",
    "    for layer in self.layers:\n",
    "        x = layer(x, x, x, mask)\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "#### **参数解释**\n",
    "- `x`: 输入序列，形状为 `(N, seq_length)`，其中 `N` 是批次大小，`seq_length` 是序列长度。\n",
    "- `mask`: 掩码，用于防止某些位置的信息泄露（如在自注意力中屏蔽未来信息）。\n",
    "\n",
    "#### **流程解释**\n",
    "1. **词嵌入与位置编码**\n",
    "   ```python\n",
    "   x = self.dropout(self.position_embedding(self.word_embedding(x)))\n",
    "   ```\n",
    "   - 首先，将输入序列 `x` 通过词嵌入层（`word_embedding`）得到嵌入向量。\n",
    "   - 然后，将嵌入向量与位置编码（`position_embedding`）相加，为每个标记添加位置信息。\n",
    "   - 最后，应用 Dropout 防止过拟合。\n",
    "\n",
    "2. **逐层传递**\n",
    "   ```python\n",
    "   for layer in self.layers:\n",
    "       x = layer(x, x, x, mask)\n",
    "   ```\n",
    "   - 输入序列 `x` 逐层传递到每个 Transformer 块中。在每个块中：\n",
    "     - `value`、`key` 和 `query` 都是 `x`，因为这是自注意力机制（Self-Attention）。\n",
    "     - `mask` 用于控制哪些位置的信息可以被关注。\n",
    "   - 每个 Transformer 块的输出会作为下一层的输入。\n",
    "\n",
    "3. **返回结果**\n",
    "   ```python\n",
    "   return x\n",
    "   ```\n",
    "   - 最终返回编码器的输出，形状为 `(N, seq_length, embed_size)`，表示每个位置的上下文表示。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **总结**\n",
    "这段代码实现了一个完整的 Transformer 编码器，其主要功能包括：\n",
    "1. **词嵌入与位置编码**：将输入标记转换为嵌入向量，并添加位置信息。\n",
    "2. **多层 Transformer 块**：通过多头注意力机制和前馈网络逐层处理输入序列。\n",
    "3. **掩码机制**：通过掩码控制注意力的范围，避免信息泄露。\n",
    "4. **Dropout 正则化**：防止过拟合。\n",
    "\n",
    "编码器的输出是一个上下文表示，可以被解码器用于生成目标序列。这种架构在机器翻译、文本生成等任务中表现出色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 解码器\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了 Transformer 解码器中的一个基本模块——`DecoderBlock`。每个 `DecoderBlock` 包含两个主要部分：**掩码多头自注意力（Masked Multi-Head Self-Attention）** 和 **Transformer 块（TransformerBlock）**。以下是对代码的详细解析：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **`DecoderBlock` 的结构**\n",
    "\n",
    "#### （1）**掩码多头自注意力（`MultiHeadAttention`）**\n",
    "```python\n",
    "self.attention = MultiHeadAttention(embed_size, heads)\n",
    "```\n",
    "- **作用**：这部分实现了掩码多头自注意力机制。\n",
    "- **输入**：`x`（解码器的输入序列）。\n",
    "- **掩码**：`trg_mask`（目标序列的掩码，用于防止解码器看到未来的信息）。\n",
    "- **输出**：经过掩码多头自注意力处理后的张量。\n",
    "\n",
    "#### （2）**归一化（`nn.LayerNorm`）**\n",
    "```python\n",
    "self.norm = nn.LayerNorm(embed_size)\n",
    "```\n",
    "- **作用**：对注意力输出进行归一化，以稳定训练。\n",
    "- **输入**：`attention + x`（注意力输出与输入的残差连接）。\n",
    "- **输出**：归一化后的张量。\n",
    "\n",
    "#### （3）**Transformer 块（`TransformerBlock`）**\n",
    "```python\n",
    "self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "```\n",
    "- **作用**：这部分实现了标准的 Transformer 块，包含多头注意力和前馈网络。\n",
    "- **输入**：\n",
    "  - `value` 和 `key`：来自编码器的输出（编码器的上下文信息）。\n",
    "  - `query`：经过归一化的解码器输入。\n",
    "  - `src_mask`：编码器的掩码（用于忽略填充部分）。\n",
    "- **输出**：经过 Transformer 块处理后的张量。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **`DecoderBlock` 的前向传播（`forward` 方法）**\n",
    "```python\n",
    "def forward(self, x, value, key, src_mask, trg_mask):\n",
    "    attention = self.attention(x, x, x, trg_mask)\n",
    "    query = self.dropout(self.norm(attention + x))\n",
    "    out = self.transformer_block(value, key, query, src_mask)\n",
    "    return out\n",
    "```\n",
    "\n",
    "#### （1）**掩码多头自注意力**\n",
    "```python\n",
    "attention = self.attention(x, x, x, trg_mask)\n",
    "```\n",
    "- **输入**：\n",
    "  - `x`：解码器的输入序列（通常是目标序列的嵌入表示）。\n",
    "  - `trg_mask`：目标序列的掩码，用于防止解码器看到未来的信息。\n",
    "- **作用**：计算解码器内部的自注意力，同时通过掩码避免看到未来的信息。\n",
    "- **输出**：经过掩码多头自注意力处理后的张量。\n",
    "\n",
    "#### （2）**归一化和残差连接**\n",
    "```python\n",
    "query = self.dropout(self.norm(attention + x))\n",
    "```\n",
    "- **作用**：\n",
    "  - 将注意力输出与输入 `x` 进行残差连接（`attention + x`），以保留输入的信息。\n",
    "  - 使用 `LayerNorm` 对残差连接的结果进行归一化。\n",
    "  - 应用 `Dropout` 进行正则化。\n",
    "- **输出**：归一化后的张量，作为查询（`query`）传递到下一个模块。\n",
    "\n",
    "#### （3）**Transformer 块**\n",
    "```python\n",
    "out = self.transformer_block(value, key, query, src_mask)\n",
    "```\n",
    "- **输入**：\n",
    "  - `value` 和 `key`：来自编码器的输出（编码器的上下文信息）。\n",
    "  - `query`：经过归一化的解码器输入。\n",
    "  - `src_mask`：编码器的掩码（用于忽略填充部分）。\n",
    "- **作用**：结合编码器的上下文信息和解码器的查询，通过 Transformer 块进行进一步处理。\n",
    "- **输出**：经过 Transformer 块处理后的张量。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **`DecoderBlock` 的作用**\n",
    "`DecoderBlock` 是 Transformer 解码器的核心模块，它结合了以下两个主要功能：\n",
    "1. **掩码多头自注意力**：\n",
    "   - 用于处理解码器内部的序列，同时通过掩码避免看到未来的信息。\n",
    "2. **编码器-解码器注意力**：\n",
    "   - 通过 `TransformerBlock`，结合编码器的上下文信息（`value` 和 `key`）和解码器的查询（`query`），生成最终的输出。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **总结**\n",
    "`DecoderBlock` 是 Transformer 解码器中的一个模块，它包含：\n",
    "- **掩码多头自注意力**：处理解码器内部的序列。\n",
    "- **归一化和残差连接**：稳定训练并保留输入信息。\n",
    "- **Transformer 块**：结合编码器的上下文信息和解码器的查询，生成最终输出。\n",
    "\n",
    "通过多个 `DecoderBlock` 的堆叠，解码器能够逐步生成目标序列，同时利用编码器提供的上下文信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = PositionalEncoding(embed_size, dropout, max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        x = self.dropout(self.position_embedding(self.word_embedding(x)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PositionalEncoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 超参数和模型实例化\u001b[39;00m\n\u001b[0;32m     30\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_vocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_vocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_pad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_pad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_expansion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 模拟数据\u001b[39;00m\n\u001b[0;32m     34\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size, num_layers, forward_expansion, heads, dropout, max_length, device)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, forward_expansion\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Transformer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_expansion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_pad_idx \u001b[38;5;241m=\u001b[39m src_pad_idx\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[1;34m(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(src_vocab_size, embed_size)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m(embed_size, dropout, max_length)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m     11\u001b[0m     [\n\u001b[0;32m     12\u001b[0m         TransformerBlock(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     ]\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PositionalEncoding' is not defined"
     ]
    }
   ],
   "source": [
    "# Transformer 模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0.1, max_length=100, device=\"cuda\"):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
    "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n",
    "\n",
    "# 超参数和模型实例化\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(src_vocab_size=10000, trg_vocab_size=10000, src_pad_idx=0, trg_pad_idx=0, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0.1, max_length=100, device=device)\n",
    "\n",
    "# 模拟数据\n",
    "src = torch.tensor([[1, 5, 6, 2, 3, 0], [4, 3, 2, 9, 0, 0]], dtype=torch.long).to(device)\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 2], [1, 5, 6, 2, 0, 0]], dtype=torch.long).to(device)\n",
    "\n",
    "out = model(src, trg[:, :-1])\n",
    "print(out.shape)  # 输出形状应为 (batch_size, trg_len - 1, trg_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 代码说明：\n",
    "1. **位置编码（`PositionalEncoding`）**：为输入序列添加位置信息。\n",
    "2. **多头注意力（`MultiHeadAttention`）**：实现多头注意力机制。\n",
    "3. **Transformer 块（`TransformerBlock`）**：包含多头注意力和前馈网络。\n",
    "4. **编码器（`Encoder`）**：由多个 Transformer 块组成。\n",
    "5. **解码器（`Decoder`）**：由多个解码器块组成，每个块包含多头注意力和 Transformer 块。\n",
    "6. **Transformer 模型（`Transformer`）**：整合编码器和解码器，并生成掩码。\n",
    "\n",
    "### 注意事项：\n",
    "- 代码中使用了 `torch.tril` 来生成解码器的掩码，防止解码器看到未来的信息。\n",
    "- 输入数据需要进行适当的预处理，包括词汇表的构建和填充标记的处理。\n",
    "- 该代码是一个基础实现，可以根据具体任务进行调整和优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个代码实现了一个完整的 Transformer 模型，包括编码器（Encoder）和解码器（Decoder），用于序列到序列的任务（如机器翻译）。以下是对模型结构和参数量的详细解析。\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **模型结构解析**\n",
    "\n",
    "#### （1）**位置编码（`PositionalEncoding`）**\n",
    "- **作用**：为输入的嵌入向量添加位置信息，使得 Transformer 能够感知序列中的位置。\n",
    "- **结构**：\n",
    "  - 使用正弦和余弦函数生成位置编码矩阵。\n",
    "  - 位置编码的形状为 `(max_len, d_model)`。\n",
    "  - 通过 `torch.sin` 和 `torch.cos` 计算每个位置的编码。\n",
    "  - 使用 `nn.Dropout` 对位置编码后的嵌入向量进行正则化。\n",
    "- **参数量**：无学习参数（位置编码是固定的，不参与训练）。\n",
    "\n",
    "#### （2）**多头注意力（`MultiHeadAttention`）**\n",
    "- **作用**：将输入分割成多个“头”，每个头学习不同的特征，然后将结果合并。\n",
    "- **结构**：\n",
    "  - `self.values`、`self.keys`、`self.queries`：三个线性层，分别将输入转换为值（Values）、键（Keys）和查询（Queries）。\n",
    "  - `self.fc_out`：线性层，将多头的输出拼接后转换回原始嵌入维度。\n",
    "- **参数量**：\n",
    "  - 每个线性层的参数量为 \\(d_{model} \\times d_{model}\\)。\n",
    "  - 总参数量：\\(4 \\times d_{model}^2\\)。\n",
    "\n",
    "#### （3）**Transformer 块（`TransformerBlock`）**\n",
    "- **作用**：包含多头注意力和前馈网络（Feed-Forward Network）。\n",
    "- **结构**：\n",
    "  - **多头注意力**：通过 `MultiHeadAttention` 实现。\n",
    "  - **LayerNorm**：归一化层，用于稳定训练。\n",
    "  - **前馈网络**：包含两个线性层，中间有 ReLU 激活函数。\n",
    "  - **Dropout**：用于正则化。\n",
    "- **参数量**：\n",
    "  - 多头注意力：\\(4 \\times d_{model}^2\\)。\n",
    "  - 前馈网络：\n",
    "    - 第一个线性层：\\(d_{model} \\times (d_{model} \\times forward\\_expansion)\\)。\n",
    "    - 第二个线性层：\\((d_{model} \\times forward\\_expansion) \\times d_{model}\\)。\n",
    "  - 总参数量：\\(4 \\times d_{model}^2 + 2 \\times d_{model}^2 \\times forward\\_expansion\\)。\n",
    "\n",
    "#### （4）**编码器（`Encoder`）**\n",
    "- **作用**：对输入序列进行编码。\n",
    "- **结构**：\n",
    "  - **词嵌入**：`nn.Embedding`，将输入的单词索引转换为嵌入向量。\n",
    "  - **位置编码**：通过 `PositionalEncoding` 添加位置信息。\n",
    "  - **多层 Transformer 块**：包含多个 `TransformerBlock`。\n",
    "- **参数量**：\n",
    "  - 词嵌入：\\(src\\_vocab\\_size \\times d_{model}\\)。\n",
    "  - 每个 Transformer 块：\\(4 \\times d_{model}^2 + 2 \\times d_{model}^2 \\times forward\\_expansion\\)。\n",
    "  - 总参数量：\\(src\\_vocab\\_size \\times d_{model} + num\\_layers \\times (4 \\times d_{model}^2 + 2 \\times d_{model}^2 \\times forward\\_expansion)\\)。\n",
    "\n",
    "#### （5）**解码器（`Decoder`）**\n",
    "- **作用**：根据编码器的输出生成目标序列。\n",
    "- **结构**：\n",
    "  - **词嵌入**：`nn.Embedding`，将目标单词索引转换为嵌入向量。\n",
    "  - **位置编码**：通过 `PositionalEncoding` 添加位置信息。\n",
    "  - **多层 Decoder 块**：包含多个 `DecoderBlock`，每个块包含多头注意力和 Transformer 块。\n",
    "  - **输出层**：`nn.Linear`，将解码器的输出转换为目标词汇表大小。\n",
    "- **参数量**：\n",
    "  - 词嵌入：\\(trg\\_vocab\\_size \\times d_{model}\\)。\n",
    "  - 每个 Decoder 块：\\(4 \\times d_{model}^2 + 2 \\times d_{model}^2 \\times forward\\_expansion\\)。\n",
    "  - 输出层：\\(d_{model} \\times trg\\_vocab\\_size\\)。\n",
    "  - 总参数量：\\(trg\\_vocab\\_size \\times d_{model} + num\\_layers \\times (4 \\times d_{model}^2 + 2 \\times d_{model}^2 \\times forward\\_expansion) + d_{model} \\times trg\\_vocab\\_size\\)。\n",
    "\n",
    "#### （6）**Transformer 模型（`Transformer`）**\n",
    "- **作用**：整合编码器和解码器，完成序列到序列的任务。\n",
    "- **结构**：\n",
    "  - 编码器：`Encoder`。\n",
    "  - 解码器：`Decoder`。\n",
    "  - 掩码生成：`make_src_mask` 和 `make_trg_mask`，用于生成编码器和解码器的掩码。\n",
    "- **参数量**：\n",
    "  - 编码器和解码器的参数量之和。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **参数量计算**\n",
    "假设：\n",
    "- `src_vocab_size = 10000`\n",
    "- `trg_vocab_size = 10000`\n",
    "- `d_model = 256`\n",
    "- `num_layers = 6`\n",
    "- `forward_expansion = 4`\n",
    "- `heads = 8`\n",
    "\n",
    "#### （1）**位置编码**\n",
    "- 无学习参数。\n",
    "\n",
    "#### （2）**多头注意力**\n",
    "- 参数量：\\(4 \\times d_{model}^2 = 4 \\times 256^2 = 262144\\)。\n",
    "\n",
    "#### （3）**Transformer 块**\n",
    "- 前馈网络：\n",
    "  - 第一个线性层：\\(d_{model} \\times (d_{model} \\times forward\\_expansion) = 256 \\times (256 \\times 4) = 262144\\)。\n",
    "  - 第二个线性层：\\((d_{model} \\times forward\\_expansion) \\times d_{model} = (256 \\times 4) \\times 256 = 262144\\)。\n",
    "- 总参数量：\\(4 \\times 256^2 + 2 \\times 262144 = 786432\\)。\n",
    "\n",
    "#### （4）**编码器**\n",
    "- 词嵌入：\\(src\\_vocab\\_size \\times d_{model} = 10000 \\times 256 = 2560000\\)。\n",
    "- 每个 Transformer 块：\\(786432\\)。\n",
    "- 总参数量：\\(2560000 + 6 \\times 786432 = 7218592\\)。\n",
    "\n",
    "#### （5）**解码器**\n",
    "- 词嵌入：\\(trg\\_vocab\\_size \\times d_{model} = 10000 \\times 256 = 2560000\\)。\n",
    "- 每个 Decoder 块：\\(786432\\)。\n",
    "- 输出层：\\(d_{model} \\times trg\\_vocab\\_size = 256 \\times 10000 = 2560000\\)。\n",
    "- 总参数量：\\(2560000 + 6 \\times 786432 + 2560000 = 7218592\\)。\n",
    "\n",
    "#### （6）**Transformer 模型**\n",
    "- 总参数量：\\(7218592 + 7218592 = 14437184\\)。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **模型结构总结**\n",
    "这个 Transformer 模型包含以下主要部分：\n",
    "1. **编码器（Encoder）**：\n",
    "   - 词嵌入和位置编码。\n",
    "   - 多层 Transformer 块，每层包含多头注意力和前馈网络。\n",
    "2. **解码器（Decoder）**：\n",
    "   - 词嵌入和位置编码。\n",
    "   - 多层 Decoder 块，每块包含多头注意力和 Transformer 块。\n",
    "   - 输出层将解码器的输出转换为目标词汇表大小。\n",
    "3. **掩码生成**：\n",
    "   - 编码器掩码：忽略填充部分。\n",
    "   - 解码器掩码：忽略未来信息和填充部分。\n",
    "\n",
    "### 4. **参数量总结**\n",
    "- **编码器参数量**：约 721 万。\n",
    "- **解码器参数量**：约 721 万。\n",
    "- **总参数量**：约 1443 万。\n",
    "\n",
    "这个模型是一个典型的 Transformer 架构，适用于机器翻译、文本生成等任务。通过多头注意力机制和前馈网络，模型能够有效地捕捉序列中的长距离依赖关系。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
