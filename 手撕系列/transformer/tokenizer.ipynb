{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义 Tokenizer 类\n",
    "我们将实现一个简单的字符级 Tokenizer，它包括以下功能：\n",
    "\n",
    "- 构建词汇表（Vocabulary）。\n",
    "- 将文本转换为索引序列。\n",
    "- 将索引序列转换回文本。\n",
    "- 添加特殊标记（如 <PAD>、<UNK>）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CharTokenizer:\n",
    "    \"\"\"\n",
    "    一个简单的字符级分词器（Tokenizer），用于将文本数据转换为数值化的索引序列。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, special_tokens=[\"<PAD>\", \"<UNK>\"]):\n",
    "        \"\"\"\n",
    "        初始化 Tokenizer。\n",
    "\n",
    "        :param special_tokens: 特殊标记列表，如填充标记（<PAD>）和未知标记（<UNK>）。\n",
    "        \"\"\"\n",
    "        # 初始化特殊标记\n",
    "        self.special_tokens = special_tokens\n",
    "        # 构建初始词汇表，将特殊标记加入词汇表\n",
    "        self.vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "        # 当前词汇表大小\n",
    "        self.vocab_size = len(self.special_tokens)\n",
    "        # 反向映射（索引到标记）暂未初始化\n",
    "        self.idx_to_token = None\n",
    "\n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        根据提供的文本数据构建词汇表。\n",
    "\n",
    "        :param texts: 文本列表，用于构建词汇表。\n",
    "        \"\"\"\n",
    "        # 遍历所有文本\n",
    "        for text in texts:\n",
    "            # 遍历文本中的每个字符\n",
    "            for char in text:\n",
    "                # 如果字符不在当前词汇表中，则添加到词汇表\n",
    "                if char not in self.vocab:\n",
    "                    self.vocab[char] = len(self.vocab)\n",
    "        # 更新词汇表大小\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        # 构建反向映射（索引到标记）\n",
    "        self.idx_to_token = {idx: token for token, idx in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text, max_length=None, padding=False, truncation=False):\n",
    "        \"\"\"\n",
    "        将文本转换为索引序列。\n",
    "\n",
    "        :param text: 输入文本。\n",
    "        :param max_length: 最大序列长度。\n",
    "        :param padding: 是否填充到最大长度。\n",
    "        :param truncation: 是否截断超出最大长度的部分。\n",
    "        :return: 索引序列（Tensor）。\n",
    "        \"\"\"\n",
    "        # 将文本拆分为字符\n",
    "        tokens = [char for char in text]\n",
    "        # 将字符映射为索引，未知字符映射为 <UNK> 的索引\n",
    "        indices = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "        # 如果指定了最大长度\n",
    "        if max_length is not None:\n",
    "            # 如果启用截断，则截断超出最大长度的部分\n",
    "            if truncation:\n",
    "                indices = indices[:max_length]\n",
    "            # 如果启用填充，则将序列填充到最大长度\n",
    "            if padding:\n",
    "                indices += [self.vocab[\"<PAD>\"]] * (max_length - len(indices))\n",
    "\n",
    "        # 将索引列表转换为 PyTorch Tensor\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"\n",
    "        将索引序列转换回文本。\n",
    "\n",
    "        :param indices: 索引序列（Tensor 或列表）。\n",
    "        :return: 文本。\n",
    "        \"\"\"\n",
    "        # 将索引序列中的每个索引映射回对应的字符\n",
    "        return \"\".join(self.idx_to_token[idx.item()] for idx in indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回词汇表大小。\n",
    "        \"\"\"\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. **代码解释**\n",
    "#### **初始化**\n",
    "- `special_tokens`：定义了特殊的标记，如 `<PAD>`（填充）和 `<UNK>`（未知字符）。\n",
    "- `vocab`：词汇表，将字符映射为索引。\n",
    "- `vocab_size`：词汇表大小。\n",
    "\n",
    "#### **构建词汇表**\n",
    "- `fit` 方法：遍历所有文本，将每个字符加入词汇表。如果字符已存在，则跳过。\n",
    "\n",
    "#### **编码**\n",
    "- `encode` 方法：\n",
    "  - 将文本拆分为字符。\n",
    "  - 将字符映射为索引，未知字符映射为 `<UNK>` 的索引。\n",
    "  - 根据 `max_length` 进行填充或截断。\n",
    "\n",
    "#### **解码**\n",
    "- `decode` 方法：将索引序列转换回文本。\n",
    "\n",
    "#### **特殊功能**\n",
    "- `padding`：将序列填充到指定长度。\n",
    "- `truncation`：将序列截断到指定长度。\n",
    "\n",
    "### 4. **扩展**\n",
    "上述实现是一个简单的字符级 Tokenizer。你可以根据需要扩展它，例如：\n",
    "- 支持子词分词（如 BPE 或 WordPiece）。\n",
    "- 添加更多特殊标记（如 `[CLS]`、`[SEP]`）。\n",
    "- 支持并行化处理大量文本。\n",
    "\n",
    "### 5. **总结**\n",
    "从零开始实现一个 Tokenizer 是一个很好的学习过程。虽然 PyTorch 本身没有内置的 Tokenizer，但你可以通过简单的 Python 代码实现一个基本的分词器。对于更复杂的任务，建议使用现成的库（如 Hugging Face 的 `transformers`），它们提供了高效且功能丰富的 Tokenizer 实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用toenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 23\n",
      "Encoded: tensor([ 2,  3,  4,  4,  5,  6, 10, 11, 12,  5,  8, 13,  2,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n",
      "Decoded: hello pytorch<PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "# 示例文本数据\n",
    "texts = [\"hello world\", \"pytorch is great\", \"tokenizer example\"]\n",
    "\n",
    "# 初始化 Tokenizer\n",
    "tokenizer = CharTokenizer()\n",
    "\n",
    "# 构建词汇表\n",
    "tokenizer.fit(texts)\n",
    "\n",
    "# 打印词汇表大小\n",
    "print(f\"Vocabulary Size: {len(tokenizer)}\")\n",
    "\n",
    "# 编码示例\n",
    "encoded = tokenizer.encode(\"hello pytorch\", max_length=20, padding=True, truncation=True)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "\n",
    "# 解码示例\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
