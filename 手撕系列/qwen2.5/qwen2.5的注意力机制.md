# qwen2.5的的注意力机制

## 1.传统的多头注意力注意力机制（HMA）

说到注意力机制就不得不提到Q、K、V三板斧。多头注意力机制通过将输入分割为多个头，然后对每个头进行自注意力，最后再进行合并。实现多头注意力机制。

具体可以看：[最适合小白入门的Transformer介绍](https://blog.csdn.net/2302_80236633/article/details/145813364?spm=1001.2014.3001.5502)

在**传统的注意力机制中**，我们会为**每个头来分配一个Q、K、V矩阵**，对于每个头都有**独属**于自己的Q、K、V矩阵。
对于$h_1$来说，会为它分配$Q_1$、$K_1$、$V_1$是三个**不同**的矩阵，对于$h_2$来说，也会分配$Q_2$、$K_2$、$V_2$三个**不同的矩阵**。

### 1.1头的数量

对于Qwen2模型的注意力头数量：

1. **Qwen2-0.5B**：
   - 总注意力头数（`num_attention_heads`）：14。
  
2. **Qwen2-7B**：
   - 总注意力头数（`num_attention_heads`）：28。

3. **Qwen2-32B**：
   - 总注意力头数（`num_attention_heads`）：40。

可以看到对于大模型来说，如果每一个**注意力头**分配的**Q、K、V**矩阵，消耗的内存和算力实在太多了，这时就有人问了，博主博主，有没有消耗**更少的内存和算力**的方法呢？有的，有的兄弟。
那就是我们要介绍的**多查询注意力（Multi Query Attention, MQA）**

## 2.多查询注意力（Multi Query Attention, MQA）

传统 **多头注意力机制（HMA）** 会为每个头分配一个Q、K、V矩阵，对于每个头都有独属于自己的Q、K、V矩阵。**多查询注意力（Multi Query Attention, MQA）** 则对其进行了优化。他的主要的改进点是**共享K、V矩阵**，即多个头共享同一个K、V矩，Q矩阵则**不共享**。这也是它名字**多查询注意力**的由来。

- **在 MQA 中**，所有的注意力头共享相同的键（K）和值（V）矩阵。每个注意力头只保留自己的查询（Q）矩阵。这意味着键和值的计算和存储只需要进行一次，而不是为每个头重复计算。
- **查询（Q）矩阵**仍然是独立的：每个头有自己独立的查询矩阵，因此每个头仍然可以从不同的角度来关注输入数据的不同特征。

这样就不用为每个头单独计算K、V矩阵了，从而节省了内存和算力。

### 2.2 优势

- **显存占用显著减少：** 在传统的多头注意力机制中，每个注意力头都独立生成 K 和 V 矩阵，随着注意力头数的增加，显存开销会快速增长。MQA 中所有注意力头共享 K 和 V 矩阵，因此不再需要为每个头独立存储这些矩阵。这在处理长序列输入时尤其有效，因为矩阵的大小与输入序列长度呈二次增长。

- **计算效率提升：** 减少了矩阵的生成和存储需求，从而减少了显存读写操作，提升了计算效率。特别是在大规模模型或长序列的情况下，显存的 带宽和读写瓶颈通常成为计算效率的主要限制。MQA 的设计直接减少了对显存的频繁读写。

- **对序列长度更友好：** 由于键和值矩阵的大小与序列长度成正比，因此减少这些矩阵的存储可以更高效地处理长序列数据。标准的 MHA 机制在序列长度很大时会显著增加存储和计算需求，而 **MQA** 有助于缓解这种问题。

### 2.3 缺点

我们说MQA也不复杂，说难听点相当于把原版的**多头注意力机制**减配了，但是我们得到的**收益仍然是正收益**，因为我们虽然削弱一点了原版的性能，但是空间上得到了**更多的提升**，所以MQA还是不错的。但是还是有人要问了，博主博主，我想全都要怎么办，有没有办法？**有的兄弟，有的**。这也是**qwen2.5的解决方案**

## 3.分组查询注意力机制（Grouped Query Attention, GQA）

是一种介于**多头注意力（MHA）** 和 **多查询注意力（MQA）** 之间的注意力机制，旨在结合两者的优点：**MHA的高质量和MQA的高效推理能力**

GQA 的**核心思想**是将 **查询（Query）**分成多个组，每个组共享一组键（Key）和值（Value），而不是每个 Query 头都独立使用自己的 Key 和 Value。具体来说，对于MQA来说，所有的头都e键和值矩阵共享，而每个头都有自己的查询矩阵。但对于与GQA来说，它将头都划分为多个组，每个组共享一组键和值矩阵，每个头都有自己的查询矩阵。

也就是说，GQA觉得MQA和MHA太极端了，它选择站中间，将MHA和MQA结合，下面是千问系列模型的GQA的参数：

1. **Qwen2-0.5B**：
   - 总注意力头数（`num_attention_heads`）：14。
   - 键值头数（`num_key_value_heads`）：2。
   - 每个查询头组包含的头数为 `num_attention_heads / num_key_value_heads`，即 7 个头共享一组键值（Key-Value）。

2. **Qwen2-7B**：
   - 总注意力头数（`num_attention_heads`）：28。
   - 键值头数（`num_key_value_heads`）：4。
   - 每个查询头组包含的头数为 7 个头共享一组键值（Key-Value）。

3. **Qwen2-32B**：
   - 总注意力头数（`num_attention_heads`）：40。
   - 键值头数（`num_key_value_heads`）：8。
   - 每个查询头组包含的头数为 5 个头共享一组键值（Key-Value）。

### 3.1.GQA 机制的优势

通过分组查询注意力机制（GQA），Qwen2 系列模型在保持较高推理效率的同时，减少了 KV 缓存的冗余计算。例如，Qwen2-32B 的 KV 缓存显存占用降低至标准多头注意力的 62%。这种设计在大规模语言模型中尤其有效，能够显著提升推理速度和资源利用率。

总结来说，Qwen2 系列模型通过调整注意力头的数量和分组策略，实现了高效的长序列处理和推理优化。
