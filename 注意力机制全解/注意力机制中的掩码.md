# 注意力机制中的掩码

在人工智能的注意力机制中，掩码（Mask）是一种重要的技术，用于控制模型对输入数据的关注范围。以下是一些常见的掩码类型：

## 1. 填充掩码（Padding Mask）

填充掩码（Padding Mask）是自然语言处理（NLP）任务中常用的一种掩码机制，主要用于处理输入序列长度不一致的问题。在实际应用中，为了方便批量处理，通常会将输入序列填充到相同的长度，而填充掩码的作用就是让模型忽略这些填充的部分。以下是关于填充掩码的详细介绍：

### 1. **填充掩码的作用**

填充掩码的主要目的是**标记输入序列中的填充部分**，并确保模型在计算注意力权重时忽略这些填充位置。如果不使用填充掩码，模型可能会将填充部分误认为是有效信息，从而影响模型的性能和学习效果。

### 2. **填充掩码的生成方法**

填充掩码通常是一个与输入序列形状相同的矩阵，其中填充部分的值为`True`（或`1`），非填充部分的值为`False`（或`0`）。以下是生成填充掩码的常见步骤：

#### **以PyTorch为例**

假设输入序列是一个二维张量`input_seq`，其中填充部分用`[PAD]`标记（通常用`0`表示）：

```python
import torch

# 输入序列
input_seq = torch.tensor([[1, 2, 0, 0], [3, 4, 5, 0]])

# 生成填充掩码
padding_mask = input_seq == 0  # 填充部分为True，非填充部分为False
print(padding_mask)

```

输出：

```python

tensor([[False, False,  True,  True],
        [False, False, False,  True]])
```

### 3. **填充掩码在注意力机制中的应用**

在注意力机制中，填充掩码通常用于修改注意力分数矩阵，使得填充部分的注意力分数被置为负无穷大（`-inf`），从而在经过softmax函数后，这些位置的权重趋近于零。

### 2. 序列掩码（Sequence Mask）

在某些任务中，为了避免模型在生成序列时看到未来的信息，需要对注意力分数进行掩码操作。Sequence mask的作用是通过构建下三角（或者上三角）的注意力分数矩阵，将当前位置之后位置的注意力分数设为一个很小的值，从而使模型只关注当前 token 与之前 token 的注意力关系，不理会它与后续 token 的关系。这样可以保证模型在生成序列时只依赖于已经生成的部分，不会受到未来信息的影响，即只”看”当前及前面的 tokens。也有把Sequence mask叫做Casual Mask的。

- **用途**：用于隐藏输入序列中的某些部分。例如，在双向模型中，可能需要根据特定标准忽略序列中的某些部分。
- **实现方式**：可以根据具体需求生成掩码，例如通过指定哪些位置需要被隐藏。

### 3. 自注意力掩码（Self-Attention Mask）

在多头注意力机制中我们在训练中需要使用**注意力掩码来遮蔽未来信息**，因为我们都是采用**自回归**的机制进行的训练，但是有小伙伴发现，我们在推理过程还是使用注意力掩码，这是为什么呢？

#### 为什么Decoder的推理过程使用到自注意力掩码

答案是为了解决**训练阶段和推理阶段的gap(不匹配)**，假如说我们有一个

- **用途**：用于控制自注意力机制中哪些位置可以相互关注。例如，在某些情况下，可能需要阻止某些位置之间的交互。
- **实现方式**：通过修改注意力分数，将需要忽略的位置的分数设置为负无穷大（`-inf`），这样在经过softmax函数后，这些位置的权重会趋近于零。
